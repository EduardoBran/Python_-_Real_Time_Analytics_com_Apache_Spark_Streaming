{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a8b6a8",
   "metadata": {},
   "source": [
    "<span style=\"color: green; font-size: 40px; font-weight: bold;\">Projeto 1 (Análise de Dados em Tempo Real) </span>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "# Análise de Dados de Sensores IoT (Internet das Coisas) em Tempo Real com Apache Spark Streaming e Apache Kafka\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Uma determinada indústria de materiais esportivos tem diversos equipamentos no parque industrial da empresa usados para produção e que funcionam 24/7.\n",
    "\n",
    "Cada equipamento tem um sensor que mede a temperatura em intervalos regulares. Os equipamentos que excedem uma determinada temperatura média por muito tempo, podem ter a vida útil reduzida, gerando custos adicionais de manutenção ou troca do equipamento.\n",
    "\n",
    "O departamento de operações gostaria de ter uma **solução de análise de dados em tempo real que calculasse a média de temperatura de cada equipamento a partir da leitura dos dados emitidos pelos sensores IoT em intervalos regulares**. Isso ajudaria no monitoramento da operação e ainda permitiria criar um histórico de uso dos equipamentos.\n",
    "\n",
    "Além de construir a solução com Spark e Kafka, vamos desenvolver um simulador para gerar dados de sensores IoT.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "O objetivo deste projeto é **demonstrar como configurar e executar uma pipeline de dados em tempo real que coleta, processa e analisa dados de sensores IoT utilizando Apache Kafka e Apache Spark Structured Streaming**. A análise se concentra em calcular a média das temperaturas reportadas por diferentes sensores em tempo real, permitindo monitorar e responder a condições específicas conforme os dados são gerados.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pergunta de Negócio Principal\n",
    "\n",
    "> A principal pergunta de negócio que este projeto visa responder é: \"**Qual é a temperatura média registrada por cada sensor IoT em tempo real, e como podemos monitorar e responder a temperaturas que excedem um determinado limiar?**\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Entregável\n",
    "\n",
    "O entregável deste projeto é uma aplicação de streaming em tempo real que:\n",
    "\n",
    "- Coleta dados de sensores IoT em tempo real usando Apache Kafka.\n",
    "- Processa e analisa esses dados em tempo real usando Apache Spark Structured Streaming.\n",
    "- Calcula e exibe a média de temperatura por sensor.\n",
    "- Permite a consulta em tempo real dos sensores que reportam temperaturas acima de um certo limite (por exemplo, acima de 65 graus Celsius).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre a Fonte de Dados\n",
    "\n",
    "Os dados utilizados no projeto são gerados por sensores IoT que monitoram a temperatura (pequenos sensores em maquinas industrias que medem a temperatura das máquinas em tempos regulares). Cada entrada de dados inclui:\n",
    "\n",
    "- **id_sensor**: Identificador único do sensor.\n",
    "- **id_equipamento**: Identificador único do equipamento ao qual o sensor está conectado.\n",
    "- **sensor**: Nome ou tipo do sensor.\n",
    "- **data_evento**: Timestamp do evento de leitura.\n",
    "- **padrao**: Objeto que encapsula as leituras do sensor, neste caso, a temperatura.\n",
    "\n",
    "#### Exemplo de Entrada de Dados (json):\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "{\n",
    "  \"id_sensor\": \"S-DSA-MP6-CAP15-02468-374DM\",\n",
    "  \"id_equipamento\": \"E-DSA-MP6-CAP15-13579-374DM\",\n",
    "  \"sensor\": \"sensor25\",\n",
    "  \"data_evento\": \"2022-11-05T15:22:16.968007Z\",\n",
    "  \"padrao\": {\n",
    "    \"formato\": \"iot:leitura:sensor:temp\",\n",
    "    \"leitura\": {\n",
    "      \"temperatura\": 42.0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Cada leitura de temperatura é capturada em um formato JSON e enviada para o tópico Kafka, que é então consumido pelo Spark Structured Streaming para análise em tempo real.\n",
    "\n",
    "#### Como simular isso?\n",
    "\n",
    "Precisamos encontrar uma forma de simular a geração de dados em tempo real a partires de sensores IoT. Para isso será necessário a **construção de um simulador usando a linguagem python para gerar dados de sensores IoT**.\n",
    "\n",
    "No dia a dia, bastaria solicitar os dados ao responsável pelas máquinas os arquivos gerados pelos sensores IoT.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Considerações Finais\n",
    "\n",
    "Este mini-projeto demonstra como é possível utilizar ferramentas modernas de big data para implementar soluções de análise em tempo real. A combinação de Apache Kafka e Apache Spark Structured Streaming oferece uma solução robusta e escalável para lidar com fluxos de dados contínuos, como os gerados por dispositivos IoT. Através desta pipeline, é possível monitorar, analisar e reagir aos dados à medida que são gerados, fornecendo insights imediatos e acionáveis para o negócio.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "# Instruções para executar o projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Etapa 1 - Simulador IoT\n",
    "\n",
    "1. Abra o terminal ou prompt de comando e acesse a pasta do projeto e vá para a pasta `simulador_iot` que contém o script `simulador.py`. Esta pasta é onde o simulador IoT está localizado, e esse script foi desenvolvido para gerar leituras simuladas de sensores IoT.\n",
    "\n",
    "> **O que o script faz:** O script `simulador.py` gera dados simulados de sensores de temperatura em formato JSON. Ele atribui valores de temperatura a sensores fictícios e os salva em um arquivo de saída. Esses dados são então usados no restante do projeto para simular um fluxo de dados IoT em tempo real.\n",
    "\n",
    "2. Execute o comando abaixo para gerar um arquivo com 10.000 leituras de sensores IoT (você pode ajustar o número de registros conforme desejar).\n",
    "\n",
    "   `python simulador.py 10000 > ../dados/dados_sensores.txt`\n",
    "\n",
    "### Etapa 2 - Apache Kafka\n",
    "\n",
    "**O que é o Apache Kafka:** \n",
    "\n",
    "O Apache Kafka é uma plataforma de streaming distribuída que permite publicar, subscrever, armazenar e processar fluxos de registros em tempo real. Neste projeto, o Kafka atua como uma ponte entre a fonte de dados (sensores IoT) e o Spark Streaming, permitindo que os dados de sensores sejam capturados e transmitidos para processamento em tempo real.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- 1. Acesse a página do Kafka e faça o download da versão usada no curso conforme mostrado na aula em vídeo.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 2. Descompacte o arquivo do Kafka dentro da pasta do Mini-Projeto 6.\n",
    "\n",
    "> **Nota:** As instruções abaixo são para MacOS e Linux. Para Windows as instruções estão no manual em pdf no Capítulo 15 do curso.\n",
    "   \n",
    "<br>\n",
    "\n",
    "- 3. Abra o **terminal 1**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para inicializar o Zookeepper (gerenciador de cluster do Kafka):\n",
    "\n",
    "   `bin/zookeeper-server-start.sh config/zookeeper.properties`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 4. Abra o **terminal 2**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para inicializar o Kafka:\n",
    "\n",
    "   `bin/kafka-server-start.sh config/server.properties`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 5. Abra o **terminal 3**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para criar um tópico no Kafka:\n",
    "\n",
    "   `bin/kafka-topics.sh --create --topic dsamp6 --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 6. No mesmo **terminal 3**, execute o comando abaixo para descrever o tópico:\n",
    "\n",
    "   `bin/kafka-topics.sh --describe --topic dsamp6 --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 7. No mesmo **terminal 3**, execute o comando abaixo para produzir o streaming de dados no Kafka (como um produtor de streaming):\n",
    "\n",
    "   `bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic dsamp6 < ../dados/dados_sensores.txt`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 8. No mesmo **terminal 3**, execute o comando abaixo para listar o conteúdo do tópico (como um consumidor de streaming):\n",
    "\n",
    "   `bin/kafka-console-consumer.sh --topic dsamp6 --from-beginning --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 9. Pressione `Ctrl+C` a qualquer momento para interromper qualquer uma das janelas. Mantenha todas elas abertas enquanto executa a Etapa 3 do projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Etapa 3 - Apache Spark\n",
    "\n",
    "1. Execute o Jupyter Notebook do projeto e execute célula a célula.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Importando Pacotes e Configurando Ambiente\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Importanto Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5fdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1aa5a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "findspark: 2.0.1\n",
      "pyspark  : 3.5.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29efd4a",
   "metadata": {},
   "source": [
    "####  Conector de integração do Spark Streaming com o Apache Kafka\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40f96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efeedac",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Criando a Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77275eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/21 18:14:23 WARN Utils: Your hostname, eduardo-Inspiron-15-3520 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface wlp0s20f3)\n",
      "24/08/21 18:14:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/eduardo/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/eduardo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/eduardo/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-304040fe-06c8-45d0-91f6-d144d3f32c51;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 300ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-304040fe-06c8-45d0-91f6-d144d3f32c51\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/6ms)\n",
      "24/08/21 18:14:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/21 18:14:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Mini-Projeto6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa5c0e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Resumo\n",
    "\n",
    "Neste trecho de código, foram realizadas as seguintes etapas essenciais para preparar o ambiente e garantir a integração entre Apache Spark e Apache Kafka para processamento de dados em tempo real:\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "1. **Inicialização do Ambiente Spark**:\n",
    "   - **findspark**: O pacote `findspark` foi importado e inicializado. Isso é necessário para que o ambiente Spark seja corretamente configurado e reconhecido dentro do Jupyter Notebook ou qualquer outro ambiente Python.\n",
    "   - **Importação de Pacotes**: Foram importados pacotes essenciais do PySpark, incluindo `StreamingContext` (para manipulação de streaming), `SparkSession` (para criação de sessões Spark), e classes relacionadas ao esquema de dados (`StructType`, `StructField`, `StringType`, `DoubleType`). Funções auxiliares como `col` e `from_json` foram importadas para manipulação de DataFrames.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Verificação de Versões**:\n",
    "   - O módulo `watermark` foi utilizado para exibir as versões dos pacotes instalados, assegurando que as versões compatíveis de `pyspark` e `findspark` estão em uso.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Integração com Apache Kafka**:\n",
    "   - **Conector Spark-Kafka**: Foi configurado o conector de integração entre Spark Streaming e Apache Kafka. A variável de ambiente `PYSPARK_SUBMIT_ARGS` foi ajustada para incluir o pacote `spark-sql-kafka-0-10_2.12:3.3.0`, que é necessário para que o Spark possa consumir streams de dados diretamente de tópicos Kafka.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Criação da Sessão Spark**:\n",
    "   - **SparkSession**: Foi criada uma sessão Spark com `SparkSession.builder.appName(\"Mini-Projeto6\").getOrCreate()`. Esta sessão é a base de todas as operações realizadas no Spark, permitindo a execução de tarefas como a leitura de streams de dados e a aplicação de operações de análise.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Log de Dependências**:\n",
    "   - Durante a inicialização da sessão Spark, o ambiente carregou as dependências necessárias, baixando pacotes da internet se necessário. Esse processo garante que todas as bibliotecas e pacotes corretos estejam disponíveis para a execução do código.\n",
    "\n",
    "<br>\n",
    "\n",
    "Este código estabelece a base para todo o processamento de dados em tempo real, conectando Apache Spark a Apache Kafka e configurando o ambiente para execução de streaming de dados. A configuração correta dessas etapas é fundamental para garantir que os dados dos sensores IoT possam ser processados em tempo real.\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "# Leitura do Stream\n",
    "\n",
    "#### Configurando a leitura de dados em tempo real a partir de um tópico Kafka utilizando Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf41f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"dsamp6\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ace9e",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "**Leitura do Stream**:\n",
    "\n",
    "Neste trecho de código, é configurada a leitura de dados em tempo real a partir de um tópico Kafka utilizando Apache Spark. \n",
    "\n",
    "1. **Configuração do Stream**:\n",
    "   - O código inicia uma subscrição para consumir dados de um tópico Kafka específico, neste caso, o tópico chamado `\"dsamp6\"`.\n",
    "   - **Método `readStream`**: Utiliza o método `spark.readStream` para configurar a leitura do stream, indicando que os dados serão lidos continuamente à medida que são produzidos pelo Kafka.\n",
    "   - **Formato Kafka**: Especifica que o formato do stream é Kafka através de `.format(\"kafka\")`, integrando diretamente o stream Kafka com o Spark.\n",
    "   - **Opções de Configuração**:\n",
    "     - `.option(\"kafka.bootstrap.servers\", \"localhost:9092\")`: Define o endereço do servidor Kafka, neste caso, localizado no `localhost` na porta `9092`.\n",
    "     - `.option(\"subscribe\", \"dsamp6\")`: Indica que o Spark deve subscrever-se ao tópico `\"dsamp6\"` para receber os dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Carregamento do Stream**:\n",
    "   - **Método `load`**: Finalmente, o método `load` é chamado para iniciar a leitura do stream de dados conforme configurado, estabelecendo a conexão entre o Spark e o Kafka para o tópico especificado.\n",
    "\n",
    "<br>\n",
    "\n",
    "Este código é crucial para estabelecer a ponte entre o Apache Kafka e o Apache Spark, permitindo que os dados dos sensores IoT, publicados no tópico Kafka `\"dsamp6\"`, sejam consumidos e processados em tempo real pelo Spark.\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# Definição do Schema:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Definindo o Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbcb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema dos dados que desejamos capturar para análise (temperatura)\n",
    "esquema_dados_temp = StructType([StructField(\"leitura\", \n",
    "                                             StructType([StructField(\"temperatura\", DoubleType(), True)]), True)])\n",
    "\n",
    "# Definimos o schema global dos dados no streaming\n",
    "esquema_dados = StructType([ \n",
    "    StructField(\"id_sensor\", StringType(), True), \n",
    "    StructField(\"id_equipamento\", StringType(), True), \n",
    "    StructField(\"sensor\", StringType(), True), \n",
    "    StructField(\"data_evento\", StringType(), True), \n",
    "    StructField(\"padrao\", esquema_dados_temp, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335d16d",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "Neste trecho de código, são definidos dois schemas fundamentais para a estruturação e análise dos dados de sensores IoT em tempo real:\n",
    "\n",
    "1. **Schema da Leitura da Temperatura (`esquema_dados_temp`)**:\n",
    "   - **Objetivo**: Modelar a estrutura do JSON especificamente para a leitura de temperatura.\n",
    "   - **Detalhes**: \n",
    "     - Define um campo chamado `leitura`, que é um objeto estruturado contendo o campo `temperatura` do tipo `DoubleType`.\n",
    "   - **Importância**: Este esquema reflete a estrutura esperada do dado de temperatura dentro do JSON, permitindo que o Spark reconheça e extraia corretamente esse valor durante o processamento.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Schema Global (`esquema_dados`)**:\n",
    "   - **Objetivo**: Modelar a estrutura completa do JSON, incluindo todos os identificadores e a leitura da temperatura.\n",
    "   - **Detalhes**:\n",
    "     - Inclui os campos `id_sensor` (identificador do sensor), `id_equipamento` (identificador do equipamento), `sensor` (nome ou tipo do sensor), `data_evento` (timestamp do evento), e `padrao`, que encapsula a estrutura de leitura da temperatura definida pelo `esquema_dados_temp`.\n",
    "   - **Importância**: Este esquema global é usado para mapear toda a estrutura do JSON que é transmitida no stream de dados. Ele garante que cada parte do dado recebido seja compreendida e organizada corretamente pelo Spark para análise subsequente.\n",
    "\n",
    "A definição desses schemas é crucial para a interpretação correta dos dados JSON recebidos via Kafka, assegurando que o Apache Spark consiga processar e analisar as informações de maneira estruturada e eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2cdb7b",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Parse e Preparo dos Dados:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Conversão de cada linha de dado do stream para JSON e transformação em um DataFrame estruturado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2deeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_sensor: string (nullable = true)\n",
      " |-- id_equipamento: string (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- data_evento: string (nullable = true)\n",
      " |-- padrao: struct (nullable = true)\n",
      " |    |-- leitura: struct (nullable = true)\n",
      " |    |    |-- temperatura: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "df_conversao = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse do formato JSON em dataframe\n",
    "df_conversao = df_conversao.withColumn(\"jsonData\", from_json(col(\"value\"), esquema_dados)).select(\"jsonData.*\")\n",
    "\n",
    "df_conversao.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d91c83",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Preparamos o Dataframe \n",
    "\n",
    "Esse dataframe está no formato que precisamos para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb302578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_conversao_temp_sensor = df_conversao.select(col(\"padrao.leitura.temperatura\").alias(\"temperatura\"), \n",
    "                                               col(\"sensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e60e042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperatura: double (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não podemos visualizar o dataframe, pois a fonte é de streaming\n",
    "# df_conversao_temp_sensor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13318b9",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "Neste trecho de código, são realizadas as operações de conversão e preparação dos dados provenientes do stream Kafka para um formato que o Spark possa analisar eficientemente:\n",
    "\n",
    "1. **Conversão para String**:\n",
    "   - **Operação**: Cada linha de dado recebida no stream é convertida para string utilizando `CAST(value AS STRING)`.\n",
    "   - **Propósito**: Como os dados no Kafka são transmitidos em formato binário, esta conversão para string é necessária para que possam ser manipulados como JSON no Spark.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Parse do JSON para DataFrame**:\n",
    "   - **Operação**: A string JSON é transformada em um DataFrame estruturado com `from_json(col(\"value\"), esquema_dados)`. A função `from_json` aplica o schema definido anteriormente (`esquema_dados`) para decompor a string JSON em colunas individuais.\n",
    "   - **Propósito**: Esta etapa permite a extração dos campos específicos do JSON, como `id_sensor`, `sensor`, `temperatura`, etc., convertendo o dado bruto em uma estrutura tabular que o Spark pode manipular diretamente.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Preparação do DataFrame**:\n",
    "   - **Seleção de Colunas**: O DataFrame resultante é refinado para incluir apenas as colunas relevantes para a análise — `temperatura` e `sensor`.\n",
    "   - **Renomeação de Colunas**: As colunas são renomeadas para simplificar as referências posteriores, por exemplo, a coluna de temperatura é renomeada como `\"temperatura\"`.\n",
    "   - **Propósito**: Essa preparação é essencial para focar a análise nos campos de interesse, facilitando cálculos como a média da temperatura por sensor.\n",
    "\n",
    "Este processo de parse e preparo transforma os dados brutos do stream em uma estrutura organizada e acessível, pronta para ser utilizada nas análises em tempo real com Apache Spark.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# Análise de Dados em Tempo Real:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Criando Objeto Para Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35e2fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- avg(temperatura): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aqui temos o objeto que irá conter nossa análise, o cálculo da média das temperaturas por sensor\n",
    "df_media_temp_sensor = df_conversao_temp_sensor.groupby(\"sensor\").mean(\"temperatura\")\n",
    "\n",
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6412b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- media_temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_media_temp_sensor = df_media_temp_sensor.select(col(\"sensor\").alias(\"sensor\"), \n",
    "                                                   col(\"avg(temperatura)\").alias(\"media_temp\"))\n",
    "\n",
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df30aa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Imprimindo o resultado no console.\n",
    "\n",
    "Abaixo abrimos o streaming para análise de dados em tempo real,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b00ef053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 11:33:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fd519894-b176-47fd-90cb-906ffc12b164. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/08/23 11:33:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/08/23 11:33:58 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/08/23 11:33:58 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/08/23 11:33:58 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/08/23 11:33:58 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/08/23 11:33:58 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+\n",
      "|sensor|media_temp|\n",
      "+------+----------+\n",
      "+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.60149253731346|\n",
      "|sensor34| 84.33309523809525|\n",
      "|sensor41| 64.60853333333334|\n",
      "|sensor50|58.499767981438545|\n",
      "|sensor31| 37.75423340961099|\n",
      "|sensor38| 57.65311720698256|\n",
      "| sensor1| 38.25805555555556|\n",
      "|sensor30| 71.98554216867474|\n",
      "|sensor10|62.726817042606584|\n",
      "|sensor25| 42.59560975609757|\n",
      "| sensor4| 73.11347150259068|\n",
      "| sensor5|   71.896675900277|\n",
      "|sensor20| 49.55213032581455|\n",
      "|sensor44| 39.85368956743001|\n",
      "|sensor19| 59.20779220779216|\n",
      "| sensor8| 51.64093023255816|\n",
      "|sensor14| 48.89083769633508|\n",
      "|sensor24|16.945177664974626|\n",
      "|sensor43|54.214245014244995|\n",
      "|sensor47| 53.32660550458717|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 77.75714285714285|\n",
      "|sensor34|            84.375|\n",
      "|sensor41| 65.17142857142858|\n",
      "|sensor50| 56.15999999999999|\n",
      "|sensor38|59.260000000000005|\n",
      "|sensor31| 38.93333333333333|\n",
      "| sensor1|             35.45|\n",
      "|sensor30| 69.47999999999999|\n",
      "|sensor10| 62.06666666666666|\n",
      "|sensor25|             42.75|\n",
      "| sensor4|             70.95|\n",
      "| sensor5|              67.1|\n",
      "|sensor20|              46.9|\n",
      "|sensor44| 40.63333333333333|\n",
      "|sensor19| 60.28000000000001|\n",
      "| sensor8|              53.5|\n",
      "|sensor14|             48.85|\n",
      "|sensor24|              16.2|\n",
      "|sensor43| 51.93333333333334|\n",
      "|sensor47|             53.05|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458877|\n",
      "|sensor34| 84.33269230769233|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694838|\n",
      "|sensor31|37.746082949308764|\n",
      "|sensor38|  57.6328282828283|\n",
      "| sensor1| 38.27374301675978|\n",
      "|sensor30| 72.01609756097565|\n",
      "|sensor10| 62.73181818181825|\n",
      "|sensor25| 42.59408866995075|\n",
      "| sensor4| 73.13612565445027|\n",
      "| sensor5| 71.90999999999998|\n",
      "|sensor20| 49.57222222222224|\n",
      "|sensor44|  39.8476923076923|\n",
      "|sensor19|59.193684210526264|\n",
      "| sensor8|51.632242990654234|\n",
      "|sensor14| 48.89105263157895|\n",
      "|sensor24| 16.95670103092784|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666668|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458877|\n",
      "|sensor34| 84.33269230769233|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694838|\n",
      "|sensor31|37.746082949308764|\n",
      "|sensor38|  57.6328282828283|\n",
      "| sensor1| 38.27374301675978|\n",
      "|sensor30| 72.01609756097564|\n",
      "|sensor10| 62.73181818181824|\n",
      "|sensor25| 42.59408866995075|\n",
      "| sensor4| 73.13612565445027|\n",
      "| sensor5|             71.91|\n",
      "|sensor20|49.572222222222244|\n",
      "|sensor44| 39.84769230769229|\n",
      "|sensor19|59.193684210526264|\n",
      "| sensor8| 51.63224299065423|\n",
      "|sensor14| 48.89105263157895|\n",
      "|sensor24| 16.95670103092784|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666669|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.55966386554624|\n",
      "|sensor34| 84.33349056603775|\n",
      "|sensor41| 64.61884816753927|\n",
      "|sensor50| 58.47293577981653|\n",
      "|sensor31| 37.76227272727273|\n",
      "|sensor38|  57.6729064039409|\n",
      "| sensor1|38.242541436464094|\n",
      "|sensor30| 71.95571428571432|\n",
      "|sensor10| 62.72189054726375|\n",
      "|sensor25| 42.59710144927537|\n",
      "| sensor4| 73.09128205128205|\n",
      "| sensor5| 71.88342541436464|\n",
      "|sensor20| 49.53233830845774|\n",
      "|sensor44| 39.85959595959594|\n",
      "|sensor19| 59.22153846153841|\n",
      "| sensor8| 51.64953703703706|\n",
      "|sensor14| 48.89062500000001|\n",
      "|sensor24|16.934000000000005|\n",
      "|sensor43|54.194915254237266|\n",
      "|sensor47|53.324090909090934|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.61571428571432|\n",
      "|sensor34| 84.33296178343952|\n",
      "|sensor41| 64.60500894454383|\n",
      "|sensor50|58.508850931677046|\n",
      "|sensor31|37.751529051987774|\n",
      "|sensor38| 57.64641068447414|\n",
      "| sensor1|38.263265306122456|\n",
      "|sensor30| 71.99564516129037|\n",
      "|sensor10| 62.72847571189286|\n",
      "|sensor25| 42.59510603588908|\n",
      "| sensor4| 73.12097053726171|\n",
      "| sensor5| 71.90110905730128|\n",
      "|sensor20| 49.55879396984926|\n",
      "|sensor44|  39.8517006802721|\n",
      "|sensor19|59.203130434782565|\n",
      "| sensor8|  51.6380434782609|\n",
      "|sensor14|  48.8909090909091|\n",
      "|sensor24| 16.94897959183674|\n",
      "|sensor43|54.220761904761886|\n",
      "|sensor47|53.327453987730074|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458877|\n",
      "|sensor34| 84.33269230769233|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694838|\n",
      "|sensor31|37.746082949308764|\n",
      "|sensor38|  57.6328282828283|\n",
      "| sensor1| 38.27374301675978|\n",
      "|sensor30| 72.01609756097564|\n",
      "|sensor10| 62.73181818181824|\n",
      "|sensor25| 42.59408866995075|\n",
      "| sensor4| 73.13612565445027|\n",
      "| sensor5|             71.91|\n",
      "|sensor20|49.572222222222244|\n",
      "|sensor44| 39.84769230769229|\n",
      "|sensor19|59.193684210526264|\n",
      "| sensor8| 51.63224299065423|\n",
      "|sensor14| 48.89105263157895|\n",
      "|sensor24| 16.95670103092784|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666669|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458879|\n",
      "|sensor34| 84.33269230769233|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694839|\n",
      "|sensor31|37.746082949308764|\n",
      "|sensor38|  57.6328282828283|\n",
      "| sensor1| 38.27374301675979|\n",
      "|sensor30| 72.01609756097565|\n",
      "|sensor10| 62.73181818181824|\n",
      "|sensor25| 42.59408866995075|\n",
      "| sensor4| 73.13612565445028|\n",
      "| sensor5| 71.90999999999998|\n",
      "|sensor20| 49.57222222222224|\n",
      "|sensor44|  39.8476923076923|\n",
      "|sensor19| 59.19368421052627|\n",
      "| sensor8| 51.63224299065423|\n",
      "|sensor14| 48.89105263157895|\n",
      "|sensor24|16.956701030927842|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666668|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de console\n",
    "query = df_media_temp_sensor.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38036866",
   "metadata": {},
   "source": [
    "## Importante\n",
    "\n",
    "- Agora é necessário abrir um **novo terminal 1** na pasta `simulador` onde está o conjunto de dados e digitar:\n",
    "\n",
    "`python simulador.py 10000 > ../dados/dados_sensores.txt`\n",
    "\n",
    "- Agora é necessário abrir um **novo terminal 2** na pasta do Kafka (`kafka_2.13-3.3.1`) e digitar:\n",
    "\n",
    "`bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic1 < ../dados/dados_sensores.txt`\n",
    "\n",
    "<br>\n",
    "\n",
    "> Após isso, a tabela acima será atualizada\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Enviando novos arquivos para o Kafka a fim de ver a análise em tempo real\n",
    "\n",
    "Clique no botão Stop no menu superior para interromper a célula a qualquer momento.\n",
    "\n",
    "**Essa janela só irá para ao apertar `stop`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eeffd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.62542735042739|\n",
      "|sensor34| 84.32559241706163|\n",
      "|sensor41| 64.62220744680852|\n",
      "|sensor50| 58.52741935483874|\n",
      "|sensor31| 37.74000000000001|\n",
      "|sensor38| 57.63640897755612|\n",
      "| sensor1|38.258653846153855|\n",
      "|sensor30| 71.99987980769234|\n",
      "|sensor10| 62.71985111662537|\n",
      "|sensor25| 42.59781021897811|\n",
      "| sensor4|  73.1200258397933|\n",
      "| sensor5| 71.88561643835615|\n",
      "|sensor20| 49.56442786069653|\n",
      "|sensor44| 39.86552598225601|\n",
      "|sensor19|59.200129198966366|\n",
      "| sensor8| 51.62906574394466|\n",
      "|sensor14| 48.89947916666667|\n",
      "|sensor24|16.950508905852423|\n",
      "|sensor43| 54.21133144475919|\n",
      "|sensor47| 53.32937142857144|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.60675105485235|\n",
      "|sensor34| 84.31733021077285|\n",
      "|sensor41| 64.63968668407311|\n",
      "|sensor50|58.525454545454565|\n",
      "|sensor31| 37.74486607142858|\n",
      "|sensor38|57.651358024691376|\n",
      "| sensor1|38.244054054054054|\n",
      "|sensor30| 71.98717339667462|\n",
      "|sensor10| 62.70565110565117|\n",
      "|sensor25| 42.60144230769232|\n",
      "| sensor4| 73.11176470588236|\n",
      "| sensor5| 71.87656675749318|\n",
      "|sensor20| 49.53423645320199|\n",
      "|sensor44|  39.8829573934837|\n",
      "|sensor19| 59.21530612244893|\n",
      "| sensor8|   51.632183908046|\n",
      "|sensor14| 48.92072538860104|\n",
      "|sensor24|16.944472361809048|\n",
      "|sensor43| 54.20140056022407|\n",
      "|sensor47| 53.32805429864255|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 0) / 200]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_148327/4070293342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Executamos a query do streaming e evitamos que o processo seja encerrado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Executamos a query do streaming e evitamos que o processo seja encerrado\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "515113c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3015841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '744d8663-f9d7-4861-9eb7-bbd84840301b',\n",
       " 'runId': 'ee4b63dd-55ee-4ce6-9e4e-9146f0f4d24a',\n",
       " 'name': None,\n",
       " 'timestamp': '2024-08-23T14:39:50.538Z',\n",
       " 'batchId': 6,\n",
       " 'numInputRows': 9472,\n",
       " 'inputRowsPerSecond': 2791.629826112585,\n",
       " 'processedRowsPerSecond': 2534.6534653465346,\n",
       " 'durationMs': {'addBatch': 3687,\n",
       "  'commitOffsets': 22,\n",
       "  'getBatch': 0,\n",
       "  'latestOffset': 1,\n",
       "  'queryPlanning': 11,\n",
       "  'triggerExecution': 3737,\n",
       "  'walCommit': 15},\n",
       " 'stateOperators': [{'operatorName': 'stateStoreSave',\n",
       "   'numRowsTotal': 50,\n",
       "   'numRowsUpdated': 50,\n",
       "   'allUpdatesTimeMs': 1014,\n",
       "   'numRowsRemoved': 0,\n",
       "   'allRemovalsTimeMs': 0,\n",
       "   'commitTimeMs': 13506,\n",
       "   'memoryUsedBytes': 101936,\n",
       "   'numRowsDroppedByWatermark': 0,\n",
       "   'numShufflePartitions': 200,\n",
       "   'numStateStoreInstances': 200,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 2400,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 30568}}],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[dsamp6]]',\n",
       "   'startOffset': {'dsamp6': {'0': 50528}},\n",
       "   'endOffset': {'dsamp6': {'0': 60000}},\n",
       "   'latestOffset': {'dsamp6': {'0': 60000}},\n",
       "   'numInputRows': 9472,\n",
       "   'inputRowsPerSecond': 2791.629826112585,\n",
       "   'processedRowsPerSecond': 2534.6534653465346,\n",
       "   'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "    'maxOffsetsBehindLatest': '0',\n",
       "    'minOffsetsBehindLatest': '0'}}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@67d6969',\n",
       "  'numOutputRows': 50}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resumo do que foi feito na última execução\n",
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2060e309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 MicroBatchWrite[epoch: 6, writer: ConsoleWriter[numRows=20, truncate=true]], org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$2371/0x0000000840fac040@787928dc\n",
      "+- *(4) HashAggregate(keys=[sensor#28], functions=[avg(temperatura#36)])\n",
      "   +- StateStoreSave [sensor#28], state info [ checkpoint = file:/tmp/temporary-fd519894-b176-47fd-90cb-906ffc12b164/state, runId = ee4b63dd-55ee-4ce6-9e4e-9146f0f4d24a, opId = 0, ver = 6, numPartitions = 200], Complete, 0, 0, 2\n",
      "      +- *(3) HashAggregate(keys=[sensor#28], functions=[merge_avg(temperatura#36)])\n",
      "         +- StateStoreRestore [sensor#28], state info [ checkpoint = file:/tmp/temporary-fd519894-b176-47fd-90cb-906ffc12b164/state, runId = ee4b63dd-55ee-4ce6-9e4e-9146f0f4d24a, opId = 0, ver = 6, numPartitions = 200], 2\n",
      "            +- *(2) HashAggregate(keys=[sensor#28], functions=[merge_avg(temperatura#36)])\n",
      "               +- Exchange hashpartitioning(sensor#28, 200), ENSURE_REQUIREMENTS, [plan_id=4289]\n",
      "                  +- *(1) HashAggregate(keys=[sensor#28], functions=[partial_avg(temperatura#36)])\n",
      "                     +- *(1) Project [jsonData#23.padrao.leitura.temperatura AS temperatura#36, jsonData#23.sensor AS sensor#28]\n",
      "                        +- Project [from_json(StructField(id_sensor,StringType,true), StructField(id_equipamento,StringType,true), StructField(sensor,StringType,true), StructField(data_evento,StringType,true), StructField(padrao,StructType(StructField(leitura,StructType(StructField(temperatura,DoubleType,true)),true)),true), cast(value#8 as string), Some(America/Sao_Paulo)) AS jsonData#23]\n",
      "                           +- MicroBatchScan[key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explica a query\n",
    "query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea7f31",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "Neste trecho de código, são realizadas as operações necessárias para calcular, exibir e monitorar a média da temperatura registrada por cada sensor IoT em tempo real utilizando Apache Spark Streaming:\n",
    "\n",
    "1. **Criação do Objeto de Análise**:\n",
    "   - **Operação**: Os dados processados são agrupados por sensor, e a média da temperatura é calculada para cada sensor usando `groupby(\"sensor\").mean(\"temperatura\")`.\n",
    "   - **Propósito**: Essa operação permite monitorar as condições térmicas de cada equipamento em tempo real, fornecendo insights sobre o comportamento dos sensores.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Renomeação das Colunas**:\n",
    "   - **Operação**: As colunas resultantes são renomeadas para simplificar a análise e tornar os resultados mais legíveis, renomeando a coluna de média de temperatura como `\"media_temp\"`.\n",
    "   - **Propósito**: A renomeação das colunas facilita a interpretação dos dados e melhora a clareza dos resultados exibidos.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Execução da Query de Streaming**:\n",
    "   - **Configuração**: A query de streaming é configurada para rodar em modo `complete`, que recalcula os resultados para todos os grupos (sensores) a cada novo batch de dados.\n",
    "   - **Exibição no Console**: O resultado é exibido em tempo real no console utilizando o método `.format(\"console\").start()`.\n",
    "   - **Propósito**: A execução da query em modo `complete` permite visualizar continuamente as médias de temperatura por sensor à medida que os dados são processados em tempo real.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Manutenção da Query Ativa**:\n",
    "   - **Operação**: O método `query.awaitTermination()` mantém a query de streaming ativa, evitando que o processo seja encerrado.\n",
    "   - **Propósito**: Isso garante que o processamento continue indefinidamente, permitindo que novos dados sejam continuamente analisados e os resultados atualizados em tempo real.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Monitoramento do Progresso da Query**:\n",
    "   - **Operações Adicionais**: Métodos como `query.status`, `query.lastProgress`, e `query.explain()` são utilizados para monitorar o status da query, visualizar o progresso dos dados processados, e explicar o plano físico da execução.\n",
    "   - **Propósito**: Essas operações são importantes para depurar e entender o comportamento da query de streaming, garantindo que ela esteja funcionando conforme o esperado.\n",
    "\n",
    "Este trecho de código é fundamental para a análise em tempo real dos dados de sensores IoT, proporcionando uma visão contínua e atualizada das condições térmicas de cada sensor. A exibição dos resultados em tempo real no console permite um monitoramento imediato, facilitando a tomada de decisões baseadas em dados ao vivo.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# Persistência e Visualização dos Resultados:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Criação de outra query de streaming configurada para manter os resultados em memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f8a9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 11:42:23 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a81f5e97-2c24-4da3-8d6e-867a3bfe389d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/08/23 11:42:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/08/23 11:42:23 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/08/23 11:42:23 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/08/23 11:42:23 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/08/23 11:42:23 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/08/23 11:42:23 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de memória (cria tabela temporária)\n",
    "query_memoria = df_media_temp_sensor \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"dsa\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b37c102e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.query.StreamingQuery at 0x7f4ad2f80070>,\n",
       " <pyspark.sql.streaming.query.StreamingQuery at 0x7f4ad2f80dc0>,\n",
       " <pyspark.sql.streaming.query.StreamingQuery at 0x7f4ad2f80910>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Streams ativados\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16b3efb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.63623813632445|\n",
      "|sensor34|  84.3328544061303|\n",
      "|sensor41| 64.60356756756757|\n",
      "|sensor50|   58.521274601687|\n",
      "|sensor31|37.749356617647074|\n",
      "|sensor38|  57.6394763343404|\n",
      "| sensor1| 38.26830357142858|\n",
      "|sensor30| 72.01245136186773|\n",
      "|sensor10|  62.7323588709678|\n",
      "|sensor25| 42.59656188605109|\n",
      "| sensor4| 73.13420502092052|\n",
      "| sensor5| 71.90999999999998|\n",
      "|sensor20|49.566532258064534|\n",
      "|sensor44| 39.85010224948875|\n",
      "|sensor19| 59.19895178197061|\n",
      "| sensor8| 51.63572761194031|\n",
      "|sensor14| 48.89505783385909|\n",
      "|sensor24|16.952049180327876|\n",
      "|sensor43| 54.22600229095073|\n",
      "|sensor47|53.328136531365324|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7|  80.6307030129125|\n",
      "|sensor34|  84.3329617834395|\n",
      "|sensor41| 64.60736086175943|\n",
      "|sensor50| 58.51731669266771|\n",
      "|sensor31| 37.75152905198778|\n",
      "|sensor38|57.643886097152446|\n",
      "| sensor1| 38.26468401486989|\n",
      "|sensor30| 72.01003236245958|\n",
      "|sensor10| 62.73271812080542|\n",
      "|sensor25| 42.59820261437909|\n",
      "| sensor4| 73.13292682926831|\n",
      "| sensor5| 71.90999999999998|\n",
      "|sensor20| 49.56275167785236|\n",
      "|sensor44|  39.8517006802721|\n",
      "|sensor19|  59.2024390243902|\n",
      "| sensor8| 51.63804347826088|\n",
      "|sensor14| 48.89772329246936|\n",
      "|sensor24| 16.94897959183674|\n",
      "|sensor43|54.220761904761886|\n",
      "|sensor47|53.327453987730074|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===========> (184 + 12) / 200][Stage 45:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458879|\n",
      "|sensor34| 84.33269230769234|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694838|\n",
      "|sensor31| 37.74608294930877|\n",
      "|sensor38|  57.6328282828283|\n",
      "| sensor1| 38.27374301675978|\n",
      "|sensor30| 72.01609756097564|\n",
      "|sensor10| 62.73181818181824|\n",
      "|sensor25| 42.59408866995075|\n",
      "| sensor4| 73.13612565445028|\n",
      "| sensor5| 71.90999999999998|\n",
      "|sensor20| 49.57222222222224|\n",
      "|sensor44|  39.8476923076923|\n",
      "|sensor19|59.193684210526264|\n",
      "| sensor8| 51.63224299065422|\n",
      "|sensor14|48.891052631578944|\n",
      "|sensor24|16.956701030927842|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666668|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458879|\n",
      "|sensor34| 84.33269230769233|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694837|\n",
      "|sensor31|37.746082949308764|\n",
      "|sensor38|  57.6328282828283|\n",
      "| sensor1| 38.27374301675978|\n",
      "|sensor30| 72.01609756097564|\n",
      "|sensor10|62.731818181818234|\n",
      "|sensor25| 42.59408866995074|\n",
      "| sensor4| 73.13612565445028|\n",
      "| sensor5|             71.91|\n",
      "|sensor20| 49.57222222222224|\n",
      "|sensor44|  39.8476923076923|\n",
      "|sensor19| 59.19368421052627|\n",
      "| sensor8| 51.63224299065422|\n",
      "|sensor14| 48.89105263157895|\n",
      "|sensor24|16.956701030927842|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666668|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458877|\n",
      "|sensor34| 84.33269230769234|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694838|\n",
      "|sensor31|37.746082949308764|\n",
      "|sensor38|  57.6328282828283|\n",
      "| sensor1| 38.27374301675978|\n",
      "|sensor30| 72.01609756097564|\n",
      "|sensor10|62.731818181818234|\n",
      "|sensor25| 42.59408866995075|\n",
      "| sensor4| 73.13612565445028|\n",
      "| sensor5|             71.91|\n",
      "|sensor20| 49.57222222222224|\n",
      "|sensor44|  39.8476923076923|\n",
      "|sensor19|59.193684210526264|\n",
      "| sensor8| 51.63224299065422|\n",
      "|sensor14|48.891052631578944|\n",
      "|sensor24|16.956701030927842|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666669|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458877|\n",
      "|sensor34| 84.33269230769234|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694837|\n",
      "|sensor31|37.746082949308764|\n",
      "|sensor38| 57.63282828282827|\n",
      "| sensor1| 38.27374301675978|\n",
      "|sensor30| 72.01609756097564|\n",
      "|sensor10| 62.73181818181823|\n",
      "|sensor25| 42.59408866995074|\n",
      "| sensor4| 73.13612565445028|\n",
      "| sensor5| 71.90999999999998|\n",
      "|sensor20| 49.57222222222224|\n",
      "|sensor44| 39.84769230769229|\n",
      "|sensor19| 59.19368421052629|\n",
      "| sensor8|51.632242990654234|\n",
      "|sensor14| 48.89105263157897|\n",
      "|sensor24|16.956701030927835|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666667|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+--------+------------------+\n",
      "|  sensor|        media_temp|\n",
      "+--------+------------------+\n",
      "| sensor7| 80.64458874458877|\n",
      "|sensor34| 84.33269230769234|\n",
      "|sensor41| 64.59782608695652|\n",
      "|sensor50| 58.52723004694838|\n",
      "|sensor31|37.746082949308764|\n",
      "|sensor38|  57.6328282828283|\n",
      "| sensor1| 38.27374301675978|\n",
      "|sensor30| 72.01609756097564|\n",
      "|sensor10|62.731818181818234|\n",
      "|sensor25| 42.59408866995075|\n",
      "| sensor4| 73.13612565445028|\n",
      "| sensor5|             71.91|\n",
      "|sensor20| 49.57222222222224|\n",
      "|sensor44| 39.84769230769229|\n",
      "|sensor19|59.193684210526264|\n",
      "| sensor8| 51.63224299065422|\n",
      "|sensor14|48.891052631578944|\n",
      "|sensor24|16.956701030927842|\n",
      "|sensor43|54.233908045976996|\n",
      "|sensor47| 53.32916666666669|\n",
      "+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos manter a query executando por algum tempo e aplicando SQL aos dados em tempo real\n",
    "from time import sleep\n",
    "\n",
    "for x in range(5):\n",
    "    \n",
    "    spark.sql(\"select sensor, round(media_temp, 2) as media from dsa where media_temp > 65\").show()\n",
    "    sleep(5)\n",
    "    \n",
    "query_memoria.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b93bd",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "Neste trecho de código, é configurada uma nova query de streaming para manter os resultados em memória, permitindo consultas SQL em tempo real sobre os dados processados:\n",
    "\n",
    "1. **Configuração da Query de Streaming em Memória**:\n",
    "   - **Operação**: Uma query de streaming é configurada para escrever os resultados em memória usando o método `.format(\"memory\")`. A query é identificada com o nome `\"dsa\"` e é executada em modo `complete`, que recalcula os resultados para todos os grupos (sensores) a cada novo batch de dados.\n",
    "   - **Propósito**: Manter os resultados em memória permite consultas SQL rápidas e eficientes sobre os dados em tempo real, facilitando a análise ad hoc e o monitoramento contínuo dos sensores.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Ativação dos Streams**:\n",
    "   - **Operação**: O método `spark.streams.active` é utilizado para verificar que as queries de streaming estão ativas, garantindo que o processamento dos dados está ocorrendo conforme esperado.\n",
    "   - **Propósito**: Esta operação confirma que o fluxo de dados está sendo processado em tempo real e que a query está preparada para fornecer resultados atualizados.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Execução de Consultas SQL em Tempo Real**:\n",
    "   - **Operação**: Um loop é configurado para executar consultas SQL a cada 3 segundos, filtrando os sensores que registram temperaturas acima de 65 graus Celsius. As consultas usam a tabela temporária `\"dsa\"` criada pela query de streaming em memória.\n",
    "   - **Propósito**: Este procedimento permite monitorar continuamente os sensores que excedem o limiar de temperatura, fornecendo insights acionáveis em tempo real.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Exibição dos Resultados**:\n",
    "   - **Resultado**: Os resultados das consultas SQL são exibidos no console, mostrando os sensores e suas respectivas médias de temperatura (`media`) que excedem o limiar especificado.\n",
    "   - **Propósito**: A exibição em tempo real permite uma rápida visualização das condições dos sensores, facilitando a identificação de potenciais problemas e a tomada de decisões rápidas.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. **Encerramento da Query**:\n",
    "   - **Operação**: Após a execução do loop de consultas, a query de streaming em memória é encerrada com `query_memoria.stop()`.\n",
    "   - **Propósito**: Isso libera recursos e encerra o processo de streaming de forma controlada após a análise.\n",
    "\n",
    "<br>\n",
    "\n",
    "Este código é essencial para permitir a persistência dos resultados em memória e a realização de consultas SQL em tempo real, proporcionando uma ferramenta poderosa para monitoramento contínuo e análise dos dados de sensores IoT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd6ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75260790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
