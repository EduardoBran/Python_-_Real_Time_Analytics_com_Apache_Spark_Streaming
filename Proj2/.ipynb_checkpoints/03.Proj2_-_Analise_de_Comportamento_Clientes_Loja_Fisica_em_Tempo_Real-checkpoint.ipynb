{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d2ecad",
   "metadata": {},
   "source": [
    "<span style=\"color: green; font-size: 40px; font-weight: bold;\">Projeto 2 (Análise de Dados em Tempo Real) </span>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "# Análise de Dados de Sensores de Movimento de Clientes em Tempo Real com Apache Spark Streaming e Apache Kafka\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Uma grande rede de lojas de varejo está interessada em entender melhor o comportamento de seus clientes dentro das lojas físicas. Cada loja está equipada com sensores de movimento instalados em diferentes áreas, como entradas, corredores e caixas. Esses sensores detectam a presença e o movimento dos clientes em tempo real.\n",
    "\n",
    "A empresa deseja ter uma **solução de análise de dados em tempo real que rastreie o fluxo de clientes dentro das lojas, calcule métricas como o tempo de permanência em cada área e a contagem de tráfego em cada seção da loja**. Esses insights ajudarão a otimizar a disposição dos produtos, melhorar o atendimento ao cliente e aumentar as vendas.\n",
    "\n",
    "Além de construir a solução com Spark e Kafka, vamos desenvolver um simulador para gerar dados de sensores de movimento em uma loja física.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "O objetivo deste projeto é **demonstrar como configurar e executar uma pipeline de dados em tempo real que coleta, processa e analisa dados de sensores de movimento utilizando Apache Kafka e Apache Spark Structured Streaming**. A análise se concentra em calcular métricas como o tempo de permanência dos clientes e o tráfego em diferentes áreas da loja, permitindo otimizar a experiência de compra e o layout da loja.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pergunta de Negócio Principal\n",
    "\n",
    "> A principal pergunta de negócio que este projeto visa responder é: \"**Quais áreas da loja recebem mais tráfego de clientes em tempo real, e como podemos otimizar a disposição dos produtos para melhorar a experiência de compra?**\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Entregável\n",
    "\n",
    "O entregável deste projeto é uma aplicação de streaming em tempo real que:\n",
    "\n",
    "- Coleta dados de sensores de movimento em tempo real usando Apache Kafka.\n",
    "- Processa e analisa esses dados em tempo real usando Apache Spark Structured Streaming.\n",
    "- Calcula e exibe métricas como o tempo de permanência por cliente e a contagem de tráfego por área.\n",
    "- Permite a consulta em tempo real das áreas da loja com maior tráfego de clientes.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre a Fonte de Dados\n",
    "\n",
    "Os dados utilizados no projeto são gerados por sensores de movimento instalados em várias áreas de uma loja física. Cada entrada de dados inclui:\n",
    "\n",
    "- **timestamp**: Data e hora exatas em que o sensor de movimento detectou um evento.\n",
    "- **id_sensor**: Identificador único do sensor de movimento.\n",
    "- **location**: Localização ou descrição da área onde o sensor está instalado dentro da loja (por exemplo, \"Entrada\", \"Corredor A\", \"Caixa\").\n",
    "- **customer_id**: Identificador único e anônimo do cliente que foi detectado pelo sensor.\n",
    "- **movement_detected**: Valor booleano indicando se o sensor detectou movimento (sempre True neste caso).\n",
    "- **duration**: Tempo, em segundos, que o cliente permaneceu na área monitorada pelo sensor antes de sair ou se mover para outra área.\n",
    "- **traffic_count**: Número de clientes que passaram pela área monitorada pelo sensor dentro de um período de tempo.\n",
    "\n",
    "#### Exemplo de Entrada de Dados (json):\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "{\n",
    "  \"timestamp\": \"2024-08-23T15:22:16.968007Z\",\n",
    "  \"id_sensor\": \"SM-123AB\",\n",
    "  \"location\": \"Corredor A\",\n",
    "  \"customer_id\": \"Cliente_5678\",\n",
    "  \"movement_detected\": true,\n",
    "  \"duration\": 152.34,\n",
    "  \"traffic_count\": 4\n",
    "}\n",
    "```\n",
    "\n",
    "Cada leitura de movimento é capturada em um formato JSON e enviada para o tópico Kafka, que é então consumido pelo Spark Structured Streaming para análise em tempo real.\n",
    "\n",
    "#### Como simular isso?\n",
    "\n",
    "Precisamos encontrar uma forma de simular a geração de dados em tempo real a partir de sensores de movimento. Para isso será necessário a construção de um simulador usando a linguagem python para gerar dados de sensores de movimento.\n",
    "\n",
    "No dia a dia, bastaria solicitar os dados ao responsável pelas lojas os arquivos gerados pelos sensores de movimento.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Considerações Finais\n",
    "\n",
    "Este mini-projeto demonstra como é possível utilizar ferramentas modernas de big data para implementar soluções de análise em tempo real. A combinação de Apache Kafka e Apache Spark Structured Streaming oferece uma solução robusta e escalável para lidar com fluxos de dados contínuos, como os gerados por sensores de movimento em lojas físicas. Através desta pipeline, é possível monitorar, analisar e reagir aos dados à medida que são gerados, fornecendo insights imediatos e acionáveis para o negócio.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# Instruções para executar o projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Etapa 1 - Simulador\n",
    "\n",
    "1. Abra o terminal ou prompt de comando e acesse a pasta do projeto e vá para a pasta `simulador` que contém o script `simulador.py`. Esta pasta é onde o simulador IoT está localizado, e esse script foi desenvolvido para gerar leituras simuladas de sensores IoT.\n",
    "\n",
    "> **O que o script faz:** O script `simulador.py` gera dados simulados de sensores de movimento em formato JSON. Ele atribui valores de movimentação a sensores fictícios e os salva em um arquivo de saída. Esses dados são então usados no restante do projeto para simular um fluxo de dados de sensores de movimento em tempo real.\n",
    "\n",
    "2. Execute o comando abaixo para gerar um arquivo com 10.000 leituras de sensores de movimento (você pode ajustar o número de registros conforme desejar).\n",
    "\n",
    "   `python simulador.py 10000 > ../dados/dados_movimento.txt`\n",
    "\n",
    "### Etapa 2 - Apache Kafka\n",
    "\n",
    "**O que é o Apache Kafka:** \n",
    "\n",
    "O Apache Kafka é uma plataforma de streaming distribuída que permite publicar, subscrever, armazenar e processar fluxos de registros em tempo real. Neste projeto, o Kafka atua como uma ponte entre a fonte de dados (sensores IoT) e o Spark Streaming, permitindo que os dados de sensores sejam capturados e transmitidos para processamento em tempo real.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- 1. Acesse a página do Kafka e faça o download da versão usada no curso conforme mostrado na aula em vídeo.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 2. Descompacte o arquivo do Kafka dentro da pasta do Mini-Projeto 6.\n",
    "\n",
    "> **Nota:** As instruções abaixo são para MacOS e Linux. Para Windows as instruções estão no manual em pdf no Capítulo 15 do curso.\n",
    "   \n",
    "<br>\n",
    "\n",
    "- 3. Abra o **terminal 1**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para inicializar o Zookeepper (gerenciador de cluster do Kafka):\n",
    "\n",
    "   `bin/zookeeper-server-start.sh config/zookeeper.properties`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 4. Abra o **terminal 2**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para inicializar o Kafka:\n",
    "\n",
    "   `bin/kafka-server-start.sh config/server.properties`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 5. Abra o **terminal 3**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para criar um tópico no Kafka \n",
    "  - (**modificar nome `topic1`**):\n",
    "\n",
    "   `bin/kafka-topics.sh --create --topic topic1 --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 6. No mesmo **terminal 3**, execute o comando abaixo para descrever o tópico:\n",
    "\n",
    "   `bin/kafka-topics.sh --describe --topic topic1 --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 7. No mesmo **terminal 3**, execute o comando abaixo para produzir o streaming de dados no Kafka (como um produtor de streaming):\n",
    "\n",
    "   `bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic1 < ../dados/dados_movimento.txt`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 8. No mesmo **terminal 3**, execute o comando abaixo para listar o conteúdo do tópico (como um consumidor de streaming):\n",
    "\n",
    "   `bin/kafka-console-consumer.sh --topic topic1 --from-beginning --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 9. Pressione `Ctrl+C` a qualquer momento para interromper qualquer uma das janelas. Mantenha todas elas abertas enquanto executa a Etapa 3 do projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Etapa 3 - Apache Spark\n",
    "\n",
    "1. Execute o Jupyter Notebook do projeto e execute célula a célula.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Importando Pacotes e Configurando Ambiente\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Importanto Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6086f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, IntegerType\n",
    "from pyspark.sql.functions import col, from_json, sum, avg, count, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29efd4a",
   "metadata": {},
   "source": [
    "####  Conector de integração do Spark Streaming com o Apache Kafka\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1caa490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efeedac",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Criando a Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "289ba113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 18:42:43 WARN Utils: Your hostname, eduardo-Inspiron-15-3520 resolves to a loopback address: 127.0.1.1; using 172.20.10.8 instead (on interface wlp0s20f3)\n",
      "24/08/23 18:42:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/eduardo/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/eduardo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/eduardo/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dd9c8693-21c4-4d75-a073-65e3e86a59da;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 505ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dd9c8693-21c4-4d75-a073-65e3e86a59da\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/9ms)\n",
      "24/08/23 18:42:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/23 18:42:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/08/23 18:42:45 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Projeto2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa5c0e",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "\n",
    "# Leitura do Stream\n",
    "\n",
    "#### Configurando a leitura de dados em tempo real a partir de um tópico Kafka utilizando Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a1fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ace9e",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# Definição do Schema:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Definindo o Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2caa4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema definido.\n"
     ]
    }
   ],
   "source": [
    "# Definimos o schema dos dados de sensores de movimento\n",
    "esquema_dados_movimento = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True), \n",
    "    StructField(\"id_sensor\", StringType(), True), \n",
    "    StructField(\"location\", StringType(), True), \n",
    "    StructField(\"customer_id\", StringType(), True), \n",
    "    StructField(\"movement_detected\", BooleanType(), True), \n",
    "    StructField(\"duration\", DoubleType(), True), \n",
    "    StructField(\"traffic_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "print('\\nSchema definido.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b425db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Resumo de tudo que foi feito\n",
    "\n",
    "1. **Simulação dos Dados e Gravação em Disco**\n",
    "\n",
    "- Primeiro, criamos um simulador que gera dados fictícios de sensores de movimento em formato JSON. Este simulador produz um conjunto de dados que imita o comportamento dos sensores em uma loja física, como o tempo em que os clientes permanecem em uma área e o número de clientes detectados. \n",
    "- O simulador cria 10.000 linhas de dados JSON, cada uma representando uma leitura de sensor. Estes dados são salvos em um arquivo de texto.\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Uso do Apache Kafka para Streaming de Dados**\n",
    "\n",
    "Em seguida, usamos o Apache Kafka para atuar como um intermediário entre os dados gerados (agora em disco) e o Apache Spark, que fará o processamento em tempo real.\n",
    "\n",
    "- **Produção de Dados**: O arquivo JSON gerado pelo simulador é enviado para um tópico Kafka, onde cada linha do arquivo é enviada como uma mensagem JSON para o Kafka.\n",
    "- **Consumo de Dados**: O Apache Spark se inscreve nesse tópico Kafka e consome essas mensagens em tempo real para processá-las.\n",
    "\n",
    "<br>\n",
    "\n",
    "3. **Definição do Schema para Processamento no Spark**\n",
    "\n",
    "Quando os dados JSON chegam ao Apache Spark através do Kafka, o Spark precisa entender a estrutura desses dados para processá-los corretamente. É aqui que entra a definição do schema.\n",
    "\n",
    "- **O que é um Schema**: O schema é como um \"molde\" que define a estrutura dos dados JSON. Ele informa ao Spark quais são os campos (colunas) presentes em cada objeto JSON e quais são os tipos de dados associados a cada campo.\n",
    "\n",
    "#### Por que precisamos de um Schema?\n",
    "\n",
    "- **Estruturação**: O schema ajuda o Spark a entender que `timestamp` é uma string que representa data e hora, `id_sensor` é uma string que identifica o sensor, `duration` é um número decimal, e assim por diante.\n",
    "- **Performance**: Com o schema, o Spark não precisa \"adivinhar\" a estrutura dos dados durante o processamento, o que torna a análise mais eficiente.\n",
    "- **Consistência**: O schema assegura que os dados sejam interpretados de forma consistente ao longo do pipeline de processamento.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. **Processamento dos Dados no Spark**\n",
    "\n",
    "Com o schema definido, o Apache Spark pode agora processar as mensagens JSON recebidas do Kafka:\n",
    "\n",
    "- **Leitura dos Dados**: O Spark lê os dados JSON e aplica o schema definido, mapeando cada campo do JSON para uma coluna correspondente.\n",
    "- **Análise**: Com os dados estruturados, o Spark pode realizar operações de análise, como calcular a média de duration (tempo que os clientes passam em uma área) ou somar o traffic_count (número de clientes em uma área)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2cdb7b",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Parse e Preparo dos Dados:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Conversão de cada linha de dado do stream para JSON e transformação em um DataFrame estruturado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8079fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- id_sensor: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- movement_detected: boolean (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- traffic_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "df_conversao = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse do formato JSON em dataframe\n",
    "df_conversao = df_conversao.withColumn(\"jsonData\", from_json(col(\"value\"), esquema_dados_movimento)).select(\"jsonData.*\")\n",
    "\n",
    "df_conversao = df_conversao.filter((col(\"duration\").isNotNull()) & (col(\"location\").isNotNull()))\n",
    "\n",
    "df_conversao.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27571b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- traffic_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionar as colunas relevantes para a análise\n",
    "df_movimento_preparado = df_conversao.select(\n",
    "    col(\"location\"),\n",
    "    col(\"duration\"),\n",
    "    col(\"traffic_count\")\n",
    ")\n",
    "\n",
    "df_movimento_preparado.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13318b9",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Análise de Dados em Tempo Real:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Criando Objeto Para Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e01c1afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location: string (nullable = true)\n",
      " |-- total_duration: double (nullable = true)\n",
      " |-- total_traffic: long (nullable = true)\n",
      " |-- avg_duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupar os dados por 'location' e calcular as métricas agregadas\n",
    "df_analise_movimento = df_movimento_preparado.groupBy(\"location\").agg(\n",
    "    sum(\"duration\").alias(\"total_duration\"),\n",
    "    sum(\"traffic_count\").alias(\"total_traffic\"),\n",
    "    avg(\"duration\").alias(\"avg_duration\")\n",
    ")\n",
    "\n",
    "# Exibir o esquema do DataFrame de análise para verificação\n",
    "df_analise_movimento.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df30aa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Imprimindo o resultado no console.\n",
    "\n",
    "Abaixo abrimos o streaming para análise de dados em tempo real,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c12ee807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/23 18:42:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b619d1ef-c54b-4afa-914b-3949945c2b37. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/08/23 18:42:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/08/23 18:42:50 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
      "24/08/23 18:42:50 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
      "24/08/23 18:42:50 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
      "24/08/23 18:42:50 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
      "24/08/23 18:42:50 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+--------------+-------------+------------+\n",
      "|location|total_duration|total_traffic|avg_duration|\n",
      "+--------+--------------+-------------+------------+\n",
      "+--------+--------------+-------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "|            location|    total_duration|total_traffic|      avg_duration|\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "|          Corredor_B|2063.7500000000005|           86| 147.4107142857143|\n",
      "|          Corredor_A|2804.4199999999996|           95| 186.9613333333333|\n",
      "|               Caixa|2340.5799999999995|          107| 137.6811764705882|\n",
      "|    Sessao_de_Roupas|1634.6099999999997|           89|125.73923076923074|\n",
      "|Sessao_de_Eletron...| 3381.460000000001|          110| 161.0219047619048|\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "|            location|    total_duration|total_traffic|      avg_duration|\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "|          Corredor_B|         309394.91|        11261| 153.0142977250247|\n",
      "|          Corredor_A|307633.29000000044|        10786|154.97898740554177|\n",
      "|               Caixa| 309626.8599999998|        10969|154.35037886340967|\n",
      "|    Sessao_de_Roupas| 301651.1699999999|        10996|152.65747469635625|\n",
      "|Sessao_de_Eletron...| 309193.3300000002|        11068|  153.751034311288|\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "|            location|    total_duration|total_traffic|      avg_duration|\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "|          Corredor_B|          314005.3|        11465|152.94948855333658|\n",
      "|          Corredor_A|313090.68000000046|        10983|155.07215453194672|\n",
      "|               Caixa| 314679.0899999998|        11170| 154.1788780009798|\n",
      "|    Sessao_de_Roupas| 305591.3499999999|        11163| 152.7193153423288|\n",
      "|Sessao_de_Eletron...| 314260.3300000002|        11254|153.59742424242432|\n",
      "+--------------------+------------------+-------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+-----------------+-------------+------------------+\n",
      "|            location|   total_duration|total_traffic|      avg_duration|\n",
      "+--------------------+-----------------+-------------+------------------+\n",
      "|          Corredor_B|618789.8200000001|        22522|153.01429772502473|\n",
      "|          Corredor_A| 615266.580000001|        21572| 154.9789874055418|\n",
      "|               Caixa|619253.7199999997|        21938| 154.3503788634097|\n",
      "|    Sessao_de_Roupas|603302.3399999999|        21992|152.65747469635625|\n",
      "|Sessao_de_Eletron...|618386.6600000004|        22136|  153.751034311288|\n",
      "+--------------------+-----------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de console\n",
    "query = df_analise_movimento.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930aa8f",
   "metadata": {},
   "source": [
    "## Importante\n",
    "\n",
    "- Agora é necessário abrir um **novo terminal 1** na pasta `simulador` onde está o conjunto de dados e digitar:\n",
    "\n",
    "`python simulador.py 10000 > ../dados/dados_movimento.txt`\n",
    "\n",
    "- Agora é necessário abrir um **novo terminal 2** na pasta do Kafka (`kafka_2.13-3.3.1`) e digitar:\n",
    "\n",
    "`bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic1 < ../dados/dados_movimento.txt`\n",
    "\n",
    "<br>\n",
    "\n",
    "> Após isso, a tabela acima será atualizada\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7e951",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# FIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
