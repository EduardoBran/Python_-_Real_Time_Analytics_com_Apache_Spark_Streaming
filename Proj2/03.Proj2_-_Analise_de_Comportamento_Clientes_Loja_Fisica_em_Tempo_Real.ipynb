{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d2ecad",
   "metadata": {},
   "source": [
    "<span style=\"color: green; font-size: 40px; font-weight: bold;\">Projeto 1 (Análise de Dados em Tempo Real) </span>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "# Análise de Dados de Sensores de Movimento de Clientes em Tempo Real com Apache Spark Streaming e Apache Kafka\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Uma grande rede de lojas de varejo está interessada em entender melhor o comportamento de seus clientes dentro das lojas físicas. Cada loja está equipada com sensores de movimento instalados em diferentes áreas, como entradas, corredores e caixas. Esses sensores detectam a presença e o movimento dos clientes em tempo real.\n",
    "\n",
    "A empresa deseja ter uma **solução de análise de dados em tempo real que rastreie o fluxo de clientes dentro das lojas, calcule métricas como o tempo de permanência em cada área e a contagem de tráfego em cada seção da loja**. Esses insights ajudarão a otimizar a disposição dos produtos, melhorar o atendimento ao cliente e aumentar as vendas.\n",
    "\n",
    "Além de construir a solução com Spark e Kafka, vamos desenvolver um simulador para gerar dados de sensores de movimento em uma loja física.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "O objetivo deste projeto é **demonstrar como configurar e executar uma pipeline de dados em tempo real que coleta, processa e analisa dados de sensores de movimento utilizando Apache Kafka e Apache Spark Structured Streaming**. A análise se concentra em calcular métricas como o tempo de permanência dos clientes e o tráfego em diferentes áreas da loja, permitindo otimizar a experiência de compra e o layout da loja.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pergunta de Negócio Principal\n",
    "\n",
    "> A principal pergunta de negócio que este projeto visa responder é: \"**Quais áreas da loja recebem mais tráfego de clientes em tempo real, e como podemos otimizar a disposição dos produtos para melhorar a experiência de compra?**\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Entregável\n",
    "\n",
    "O entregável deste projeto é uma aplicação de streaming em tempo real que:\n",
    "\n",
    "- Coleta dados de sensores de movimento em tempo real usando Apache Kafka.\n",
    "- Processa e analisa esses dados em tempo real usando Apache Spark Structured Streaming.\n",
    "- Calcula e exibe métricas como o tempo de permanência por cliente e a contagem de tráfego por área.\n",
    "- Permite a consulta em tempo real das áreas da loja com maior tráfego de clientes.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre a Fonte de Dados\n",
    "\n",
    "Os dados utilizados no projeto são gerados por sensores de movimento instalados em várias áreas de uma loja física. Cada entrada de dados inclui:\n",
    "\n",
    "- **timestamp**: Data e hora exatas em que o sensor de movimento detectou um evento.\n",
    "- **id_sensor**: Identificador único do sensor de movimento.\n",
    "- **location**: Localização ou descrição da área onde o sensor está instalado dentro da loja (por exemplo, \"Entrada\", \"Corredor A\", \"Caixa\").\n",
    "- **customer_id**: Identificador único e anônimo do cliente que foi detectado pelo sensor.\n",
    "- **movement_detected**: Valor booleano indicando se o sensor detectou movimento (sempre True neste caso).\n",
    "- **duration**: Tempo, em segundos, que o cliente permaneceu na área monitorada pelo sensor antes de sair ou se mover para outra área.\n",
    "- **traffic_count**: Número de clientes que passaram pela área monitorada pelo sensor dentro de um período de tempo.\n",
    "\n",
    "#### Exemplo de Entrada de Dados (json):\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "{\n",
    "  \"timestamp\": \"2024-08-23T15:22:16.968007Z\",\n",
    "  \"id_sensor\": \"SM-123AB\",\n",
    "  \"location\": \"Corredor A\",\n",
    "  \"customer_id\": \"Cliente_5678\",\n",
    "  \"movement_detected\": true,\n",
    "  \"duration\": 152.34,\n",
    "  \"traffic_count\": 4\n",
    "}\n",
    "```\n",
    "\n",
    "Cada leitura de movimento é capturada em um formato JSON e enviada para o tópico Kafka, que é então consumido pelo Spark Structured Streaming para análise em tempo real.\n",
    "\n",
    "#### Como simular isso?\n",
    "\n",
    "Precisamos encontrar uma forma de simular a geração de dados em tempo real a partir de sensores de movimento. Para isso será necessário a construção de um simulador usando a linguagem python para gerar dados de sensores de movimento.\n",
    "\n",
    "No dia a dia, bastaria solicitar os dados ao responsável pelas lojas os arquivos gerados pelos sensores de movimento.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Considerações Finais\n",
    "\n",
    "Este mini-projeto demonstra como é possível utilizar ferramentas modernas de big data para implementar soluções de análise em tempo real. A combinação de Apache Kafka e Apache Spark Structured Streaming oferece uma solução robusta e escalável para lidar com fluxos de dados contínuos, como os gerados por sensores de movimento em lojas físicas. Através desta pipeline, é possível monitorar, analisar e reagir aos dados à medida que são gerados, fornecendo insights imediatos e acionáveis para o negócio.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "# Instruções para executar o projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Etapa 1 - Simulador\n",
    "\n",
    "1. Abra o terminal ou prompt de comando e acesse a pasta do projeto e vá para a pasta `simulador` que contém o script `simulador.py`. Esta pasta é onde o simulador IoT está localizado, e esse script foi desenvolvido para gerar leituras simuladas de sensores IoT.\n",
    "\n",
    "> **O que o script faz:** O script `simulador.py` gera dados simulados de sensores de movimento em formato JSON. Ele atribui valores de movimentação a sensores fictícios e os salva em um arquivo de saída. Esses dados são então usados no restante do projeto para simular um fluxo de dados de sensores de movimento em tempo real.\n",
    "\n",
    "2. Execute o comando abaixo para gerar um arquivo com 10.000 leituras de sensores de movimento (você pode ajustar o número de registros conforme desejar).\n",
    "\n",
    "   `python simulador.py 10000 > ../dados/dados_movimento.txt`\n",
    "\n",
    "### Etapa 2 - Apache Kafka\n",
    "\n",
    "**O que é o Apache Kafka:** \n",
    "\n",
    "O Apache Kafka é uma plataforma de streaming distribuída que permite publicar, subscrever, armazenar e processar fluxos de registros em tempo real. Neste projeto, o Kafka atua como uma ponte entre a fonte de dados (sensores IoT) e o Spark Streaming, permitindo que os dados de sensores sejam capturados e transmitidos para processamento em tempo real.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- 1. Acesse a página do Kafka e faça o download da versão usada no curso conforme mostrado na aula em vídeo.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 2. Descompacte o arquivo do Kafka dentro da pasta do Mini-Projeto 6.\n",
    "\n",
    "> **Nota:** As instruções abaixo são para MacOS e Linux. Para Windows as instruções estão no manual em pdf no Capítulo 15 do curso.\n",
    "   \n",
    "<br>\n",
    "\n",
    "- 3. Abra o **terminal 1**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para inicializar o Zookeepper (gerenciador de cluster do Kafka):\n",
    "\n",
    "   `bin/zookeeper-server-start.sh config/zookeeper.properties`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 4. Abra o **terminal 2**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para inicializar o Kafka:\n",
    "\n",
    "   `bin/kafka-server-start.sh config/server.properties`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 5. Abra o **terminal 3**, navegue até a pasta do Kafka (`kafka_2.13-3.3.1`) e execute o comando abaixo para criar um tópico no Kafka:\n",
    "\n",
    "   `bin/kafka-topics.sh --create --topic dsamp6 --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 6. No mesmo **terminal 3**, execute o comando abaixo para descrever o tópico:\n",
    "\n",
    "   `bin/kafka-topics.sh --describe --topic dsamp6 --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 7. No mesmo **terminal 3**, execute o comando abaixo para produzir o streaming de dados no Kafka (como um produtor de streaming):\n",
    "\n",
    "   `bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic dsamp6 < ../dados/dados_movimento.txt`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 8. No mesmo **terminal 3**, execute o comando abaixo para listar o conteúdo do tópico (como um consumidor de streaming):\n",
    "\n",
    "   `bin/kafka-console-consumer.sh --topic dsamp6 --from-beginning --bootstrap-server localhost:9092`\n",
    "\n",
    "<br>\n",
    "\n",
    "- 9. Pressione `Ctrl+C` a qualquer momento para interromper qualquer uma das janelas. Mantenha todas elas abertas enquanto executa a Etapa 3 do projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Etapa 3 - Apache Spark\n",
    "\n",
    "1. Execute o Jupyter Notebook do projeto e execute célula a célula.\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Importando Pacotes e Configurando Ambiente\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Importanto Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29efd4a",
   "metadata": {},
   "source": [
    "####  Conector de integração do Spark Streaming com o Apache Kafka\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1caa490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efeedac",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Criando a Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ba113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Mini-Projeto6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa5c0e",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "\n",
    "# Leitura do Stream\n",
    "\n",
    "#### Configurando a leitura de dados em tempo real a partir de um tópico Kafka utilizando Apache Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"dsamp6\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ace9e",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# Definição do Schema:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Definindo o Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2caa4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cc737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
