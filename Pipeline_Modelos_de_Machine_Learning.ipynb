{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4ea025",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 40px; font-weight: bold;\"> Projeto  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844870b1",
   "metadata": {},
   "source": [
    "# Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fc605",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Importando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c16993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed4dcccb",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Carregando Conjunto de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479b6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc7328d1",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 34px; font-weight: bold;\"> Análise Exploratória Inicial dos Dados </span>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Função Para Análise Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed763d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função Genérica para uma Análise Inicial (para dataframes do tipo Pandas)\n",
    "\n",
    "def analise_inicial(df):\n",
    "    # Informações do DataFrame\n",
    "    print('\\n-------------------------------------------------------------------------------------------\\n\\n')\n",
    "    print('INFO\\n\\n')\n",
    "    df.info()\n",
    "\n",
    "    # Verifica se há valores ausentes e duplicados\n",
    "    valores_ausentes = df.isna().sum().sum() > 0\n",
    "    valores_duplicados = df.duplicated().sum() > 0\n",
    "\n",
    "    # Nomes das variáveis com valores ausentes\n",
    "    variaveis_ausentes = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "    # Número de linhas duplicadas\n",
    "    num_linhas_duplicadas = df.duplicated().sum()\n",
    "\n",
    "    # Porcentagem de linhas duplicadas\n",
    "    porcentagem_linhas_duplicadas = (num_linhas_duplicadas / len(df)) * 100\n",
    "\n",
    "    # Total de linhas com pelo menos um valor ausente\n",
    "    total_linhas_ausentes = df.isna().any(axis=1).sum()\n",
    "\n",
    "    # Porcentagem de linhas com pelo menos um valor ausente\n",
    "    porcentagem_linhas_ausentes = (total_linhas_ausentes / len(df)) * 100\n",
    "\n",
    "    # Exibe o resultado\n",
    "    if valores_ausentes:\n",
    "        print(\"\\n\\nExistem valores ausentes:\", valores_ausentes)\n",
    "        print(\"\\nVariáveis com valores ausentes:\", variaveis_ausentes)\n",
    "        print(\"\\nTotal de Linhas com Valores Ausentes:\", total_linhas_ausentes)\n",
    "        print(\"\\nPorcentagem de Linhas Com Valor Ausente: {:.2f}%\".format(porcentagem_linhas_ausentes))\n",
    "    else:\n",
    "        print(\"\\n\\nNenhuma variável possui valores ausentes.\")\n",
    "\n",
    "    if valores_duplicados:\n",
    "        print(\"\\n\\n\\nExistem valores duplicados:\", valores_duplicados)\n",
    "        print(\"\\nNúmero de Linhas Duplicadas:\", num_linhas_duplicadas)\n",
    "        print(\"\\nPorcentagem de Linhas Duplicadas: {:.2f}%\\n\\n\".format(porcentagem_linhas_duplicadas))\n",
    "    else:\n",
    "        print(\"\\nNenhuma variável possui valores duplicados.\\n\\n\")\n",
    "\n",
    "analise_inicial(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66244cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função Genérica para uma Análise Inicial (para dataframes do tipo PySpark)\n",
    "\n",
    "def analise_inicial(df):\n",
    "    # Exibir o schema do DataFrame\n",
    "    print('\\n\\n INFO (schema)\\n\\n')\n",
    "    df.printSchema()\n",
    "    \n",
    "    print('\\n\\n ------------------------------------------------------------------------------------------ \\n\\n')\n",
    "    \n",
    "    # Número de linhas\n",
    "    print(\"Número de Linhas:\", df.count())\n",
    "\n",
    "    # Verificar valores ausentes em cada coluna\n",
    "    print('\\n\\n TOTAL DE VALORES NaN por Coluna \\n')\n",
    "    valores_ausentes = df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns])\n",
    "    valores_ausentes.show()\n",
    "\n",
    "    # Verificar se existe alguma variável com valores ausentes\n",
    "    valores_ausentes_boolean = any(row[c] > 0 for row in valores_ausentes.collect() for c in df.columns)\n",
    "\n",
    "    # Nomes das variáveis com valores ausentes\n",
    "    variaveis_ausentes = [c for c in df.columns if df.filter(col(c).isNull() | isnan(c)).count() > 0]\n",
    "\n",
    "    # Número de linhas duplicadas\n",
    "    num_linhas_total = df.count()\n",
    "    num_linhas_distintas = df.distinct().count()\n",
    "    num_linhas_duplicadas = num_linhas_total - num_linhas_distintas\n",
    "\n",
    "    # Porcentagem de linhas duplicadas\n",
    "    porcentagem_linhas_duplicadas = (num_linhas_duplicadas / num_linhas_total) * 100\n",
    "\n",
    "    # Total de linhas com pelo menos um valor ausente\n",
    "    total_linhas_ausentes = df.filter(\n",
    "        \" OR \".join([f\"(`{c}` IS NULL OR isnan(`{c}`))\" for c in df.columns])\n",
    "    ).count()\n",
    "\n",
    "    # Porcentagem de linhas com pelo menos um valor ausente\n",
    "    porcentagem_linhas_ausentes = (total_linhas_ausentes / num_linhas_total) * 100\n",
    "\n",
    "    # Exibe o resultado\n",
    "    print(\"\\n\\nExistem valores ausentes:\", valores_ausentes_boolean)\n",
    "    \n",
    "    if valores_ausentes_boolean:\n",
    "        print(\"\\nVariáveis com valores ausentes:\", variaveis_ausentes)\n",
    "        print(\"\\nTotal de Linhas com Valores Ausentes:\", total_linhas_ausentes)\n",
    "        print(\"\\nPorcentagem de Linhas Com Valor Ausente: {:.2f}%\".format(porcentagem_linhas_ausentes))\n",
    "    else:\n",
    "        print(\"\\nNenhuma variável possui valores ausentes.\")\n",
    "\n",
    "    print(\"\\n\\n\\nExistem valores duplicados:\", num_linhas_duplicadas > 0)\n",
    "    \n",
    "    if num_linhas_duplicadas > 0:\n",
    "        print(\"\\nNúmero de Linhas Duplicadas:\", num_linhas_duplicadas)\n",
    "        print(\"\\nPorcentagem de Linhas Duplicadas: {:.2f}%\\n\".format(porcentagem_linhas_duplicadas))\n",
    "    else:\n",
    "        print(\"\\nNenhuma variável possui valores duplicados.\\n\")\n",
    "\n",
    "# Exemplo de uso da função com o DataFrame df_spark\n",
    "analise_inicial(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3008f",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b4d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79d448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f9dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ccf9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d48c461",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Visualizando os Tipos dos Dados Separadamente\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Visualizando Variáveis Categóricas e Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo Variáveis Categóricas (filtrando)\n",
    "dados.dtypes[dados.dtypes == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34228f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibindo Variáveis Numéricas (filtrando)\n",
    "dados.dtypes[dados.dtypes != 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9bf49",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Analisando Variáveis Categóricas\n",
    "\n",
    "#### Resumo Estatístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f3e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe (informando que é para somente variáveis categóricas)\n",
    "print('\\nDescribe\\n')\n",
    "display(dados.describe(include = ['object']))\n",
    "print('\\n------------------------------------------------------------------------\\n\\n')\n",
    "\n",
    "# Verificando Tipo das Variáveis (adicionar mais variáveis se necessário)\n",
    "print('\\nTipo das Variáveis\\n')\n",
    "print(dados['genre'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc2573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir os gráficos de contagem para todas as variáveis categóricas (adicionar variáveis categóricas necessárias)\n",
    "categorical_vars = ['genre']\n",
    "\n",
    "for var in categorical_vars:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=dados, x=var, label='Count')\n",
    "    plt.title(f'Contagem de {var}')\n",
    "    plt.xlabel(var.replace('_', ' ').capitalize())\n",
    "    plt.ylabel('Contagem')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Value counts para todas as variáveis categóricas\n",
    "for var in categorical_vars:\n",
    "    var_counts = dados[var].value_counts()\n",
    "    print(f\"\\nContagem de {var.replace('_', ' ').capitalize()}:\")\n",
    "    for category, count in var_counts.items():\n",
    "        print(f'{category}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a34503",
   "metadata": {},
   "source": [
    "### Resumo da Análise\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3558d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f7d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5ecfd29",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Aplicando Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de variáveis categóricas (adicionar variáveis categóricas necessárias)\n",
    "categorical_vars = ['genre']\n",
    "\n",
    "# Aplicando Label Encoding\n",
    "label_encoders = {}\n",
    "for var in categorical_vars:\n",
    "    le = LabelEncoder()\n",
    "    dados[var] = le.fit_transform(dados[var])\n",
    "    label_encoders[var] = le\n",
    "    \n",
    "# Verificando os tipos das variáveis\n",
    "print('\\nTipos das Variáveis Após Label Encoding\\n')\n",
    "print(dados.dtypes)\n",
    "\n",
    "# Verificando os valores únicos de 'income'\n",
    "print('\\n\\n-------------------------------------------------------------------------------------------------')\n",
    "print('\\nValores Únicos da Variável Alvo (genre)\\n')\n",
    "print(dados['genre'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af763322",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Analisando Todas as Variáveis\n",
    "\n",
    "#### Resumo Estatístico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f49c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o formato original\n",
    "original_float_format = pd.options.display.float_format\n",
    "\n",
    "# Ajustar a exibição do pandas para valores sem notação científica\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Verificando o resumo estatístico sem notação científica\n",
    "print('\\nSem Notação Científica')\n",
    "display(dados.describe())\n",
    "print('\\n----------------------------------------------------------------------------------------------\\n\\n')\n",
    "\n",
    "# Restaurar o formato original\n",
    "pd.options.display.float_format = original_float_format\n",
    "\n",
    "# Verificando o resumo estatístico novamente para confirmar que voltou ao normal\n",
    "#print('\\nCom Notação Científica')\n",
    "#display(dados.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4136ffe",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Visualizando através de Gráficos Histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2554f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "dados.hist(figsize = (15,15), bins = 10) \n",
    "plt.show()\n",
    "\n",
    "print('\\n\\n------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "# Visualização dos outliers\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=dados)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot para Detecção de Outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc14754",
   "metadata": {},
   "source": [
    "### Resumo da Análise\n",
    "\n",
    "<table border=\"2\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Nome Da Varivel</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Resumo_Análise</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Tratamento Antes Divisão</th>\n",
    "    <th style=\"text-align: center; font-size: 16px;\">Tratamento Depois Divisão</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>energy</td>\n",
    "    <td>Distribuição assimétrica com muitos valores altos, concentrando-se entre 0.6 e 1.0.</td>\n",
    "    <td>Remoção de Outliers</td>\n",
    "    <td>Transformação para Normalização</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>speechiness</td>\n",
    "    <td>Distribuição assimétrica, com muitos valores baixos concentrando-se perto de 0.</td>\n",
    "    <td>Remoção de Outliers</td>\n",
    "    <td>Transformação para Normalização</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>genre</td>\n",
    "    <td>Variável Alvo. Distribuição categórica dos gêneros musicais, já codificada numericamente.</td>\n",
    "    <td>Não Necessita de Tratamento</td>\n",
    "    <td>Não Necessita de Tratamento</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437923fb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Limpeza nos Dados\n",
    "\n",
    "<br>\n",
    "\n",
    "### Tratando Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2971d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73165ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cbd9c0c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tratando Valores Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359c1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aef4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32a1a9d9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tratando Valores Outliers\n",
    "\n",
    "- Apenas Remoção de Linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cb1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd854a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9fc0b63",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Features Engineering (se necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ed960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206730a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25b90497",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Clusterização\n",
    "\n",
    "- A técnica de **clusterização cria uma nova variável** que representa os clusters identificados no conjunto de dados. Esta nova variável pode ser usada como uma feature adicional na criação de modelos preditivos, fornecendo informações sobre os padrões identificados durante a clusterização.\n",
    "- Verificar **variáveis relevantes** para a clusterização.\n",
    "- Verificar no gráfico de cotovelo \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb99a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparação dos Dados\n",
    "\n",
    "# Selecionar as variáveis relevantes para a clusterização (verificar veriáveis)\n",
    "variables = ['']\n",
    "\n",
    "# Criar um DataFrame apenas com as variáveis selecionadas\n",
    "cluster_data = dados[variables]\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "\n",
    "\n",
    "## Escolha do Algoritmo de Clusterização e Avaliação do Número Ótimo de Clusters\n",
    "\n",
    "# Avaliar o método do cotovelo para encontrar o número ótimo de clusters\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(cluster_data_scaled)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Plotar o gráfico do método do cotovelo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), sse, marker='o')\n",
    "plt.title('Método do Cotovelo')\n",
    "plt.xlabel('Número de Clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09ad49",
   "metadata": {},
   "source": [
    "#### Execução da Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execução da Clusterização e Criação da Nova Variável\n",
    "\n",
    "# Escolher o número de clusters com base no gráfico do método do cotovelo (onde a cauda começa a abaixar)\n",
    "optimal_clusters = 4\n",
    "\n",
    "# Aplicar K-Means com o número ótimo de clusters\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "dados['Cluster'] = kmeans.fit_predict(cluster_data_scaled)\n",
    "\n",
    "# Verificar as novas colunas criadas\n",
    "print(\"\\nNovas colunas criadas:\")\n",
    "print(dados.columns)\n",
    "\n",
    "# Exibir uma amostra do DataFrame para verificar as novas colunas\n",
    "display(dados.head(10))\n",
    "\n",
    "# Visualizar os clusters resultantes\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(x='Total_Damages', y='Total_Affected', hue='Cluster', data=dados, palette='viridis')\n",
    "plt.title('Clusterização de Regiões e Tipos de Desastres')\n",
    "plt.xlabel('Total de Danos')\n",
    "plt.ylabel('Total de Afetados')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Mostrar a contagem de observações em cada cluster\n",
    "print(dados['Cluster'].value_counts())\n",
    "\n",
    "# Analisar as características médias de cada cluster\n",
    "cluster_summary = dados.groupby('Cluster')[variables].mean()\n",
    "display(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f6b22",
   "metadata": {},
   "source": [
    "#### Verificando Desempenho da Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calcular a pontuação de silhueta\n",
    "silhouette_avg = silhouette_score(cluster_data_scaled, dados['Cluster'])\n",
    "print(f'Pontuação de Silhueta: {silhouette_avg:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575e5ee",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Verificando Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verificando Correlações através de Tabela\n",
    "\n",
    "# Criando Tabela\n",
    "display(dados.corr())\n",
    "\n",
    "print('\\n\\n=================================================================================================\\n')\n",
    "\n",
    "\n",
    "## Visualizando Correlações através de um Mapa de Calor\n",
    "\n",
    "# Criando o Heatmap\n",
    "corr_matrix = dados.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "plt.figure(figsize=(16, 12))  # Define o tamanho da figura maior\n",
    "heatmap = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1, cbar=True, square=True, annot_kws={\"size\": 10})\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=45, horizontalalignment='right', fontsize=12)  # Aumenta a fonte das labels\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, fontsize=12)  # Aumenta a fonte das labels\n",
    "plt.title('Mapa de Calor das Correlações', fontsize=18)  # Aumenta o título\n",
    "plt.tight_layout()  # Ajusta o layout para evitar corte de labels\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\n\\n---------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "# Calcular correlação com a variável alvo\n",
    "correlation_target = dados_ml.corr()['ganhos_mensais_estimados_minimo'].drop('ganhos_mensais_estimados_minimo')\n",
    "\n",
    "# Ordenar pela correlação absoluta\n",
    "correlation_target_sorted = correlation_target.abs().sort_values(ascending=False)\n",
    "\n",
    "# Visualizar as correlações\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=correlation_target_sorted.values, y=correlation_target_sorted.index, palette='coolwarm')\n",
    "plt.title('Correlação com Ganhos Mensais Estimados Mínimos')\n",
    "plt.xlabel('Correlação')\n",
    "plt.show()\n",
    "\n",
    "# Exibir correlações\n",
    "display(correlation_target_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e182366",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Resumo da Análise:\n",
    "\n",
    "#### Análise das Principais Correlações com a Variável Alvo\n",
    "\n",
    "- \n",
    "\n",
    "#### Variáveis a serem removidas:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedc48c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Seleção de Variáveis Usando RandomForest\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005d8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93725edd",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "- Combinando a **análise de correlação** e a **análise de importância das variáveis**, a recomendação é"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafadf3",
   "metadata": {},
   "source": [
    "<br> <br> \n",
    "\n",
    "# Engenharia de Atributos\n",
    "\n",
    "- Aplicar técnica de VectorAssembler em objetos PySpark\n",
    "\n",
    "<br>\n",
    "\n",
    "### Aplicando Função VectorAssembler \n",
    "\n",
    "- Criando um **Objeto Vetor de Atributos** e adicionando ao Dataframe\n",
    "- As **colunas selecionadas** foram escolhidas a partir das análises feita anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea784e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara o vetor de atributos (Este processo combina as colunas definidas em uma única coluna vetorial)\n",
    "assembler = VectorAssembler(inputCols = ['Open', 'VolBTC', 'VolCurrency'], \n",
    "                            outputCol = \"features\")\n",
    "\n",
    "# Adicionar a nova coluna ao DataFrame\n",
    "df_assembled = assembler.transform(dados)\n",
    "\n",
    "# Verificando os Tipos das Colunas\n",
    "df_assembled.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afb56ad",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Pré-Processamento de Dados Para Construção de Modelos de Machine Learning\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## Dividindo os dados em Dados de Treino e Dados de Teste\n",
    "- Nós **treinamos** o modelo com **dados de treino** e **avaliamos** o modelo com **dados de teste**.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um objeto separado para a variável alvo\n",
    "y = dados.income\n",
    "\n",
    "# Cria um objeto separado para as variáveis de entrada\n",
    "X = dados.drop('income', axis=1)\n",
    "\n",
    "# Split em dados de treino e teste sem amostragem estratificada\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=1234)\n",
    "\n",
    "# Print do shape\n",
    "print(X_treino.shape, X_teste.shape, y_treino.shape, y_teste.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36865d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tratando Valores Outliers\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Importante**:\n",
    "Não devemos modificar a escala antes da divisão em treino e teste para evitar vazamento de dados, pois isso garantiria que informações dos dados de teste influenciem a transformação, o que poderia levar a um modelo excessivamente otimista e menos generalizável para dados não vistos.\n",
    "\n",
    "> **Importante 2**:\n",
    "A abordagem correta envolve ajustar o Tips de Tratamento de Outliers e o escalonamento nos dados de treino e aplicar esses mesmos parâmetros aos dados de teste para manter a consistência.\n",
    "\n",
    "> **Sugestão**:\n",
    "É mais apropriado tratar cada variável **individualmente**, pois cada uma possui uma distribuição e comportamento diferente. Tratando-as individualmente, podemos aplicar métodos específicos para cada caso, o que resulta em uma limpeza mais precisa e adequada para o modelo.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Tipos de Tratamento de Outliers\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Técnicas Padrão:\n",
    "\n",
    "> **Winsorização**: Limita os valores dos outliers a limites percentuais definidos, substituindo valores extremos.\n",
    "\n",
    "- **Vantagens**: Preserva todos os dados, apenas ajustando valores extremos e fácil de implementar e entender.\n",
    "- **Desvantagens**: Pode distorcer a distribuição dos dados e os limites precisam ser escolhidos com cuidado.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Remoção de Linhas**: Remove registros contendo outliers. Pode resultar na perda de dados valiosos, mas é útil quando os outliers são extremamente discrepantes.\n",
    "\n",
    "- **Vantagens**: Eficaz quando os outliers são poucos e claramente discrepantes.\n",
    "- **Desvantagens**: Pode levar à perda significativa de dados e não aplicável se muitos outliers estiverem presentes.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Capping (Truncation)**: Limita os valores dos dados a um máximo e um mínimo definidos, substituindo os outliers pelos limites.\n",
    "\n",
    "- **Vantagens**: Mantém todos os dados, evitando a remoção de registros.\n",
    "- **Desvantagens**: Pode introduzir viés ao fixar valores nos limites e os limites precisam ser escolhidos com cuidado.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Técnicas Avançadas:\n",
    "\n",
    "> **Detecção de Outliers com Isolation Forest**: Algoritmo de aprendizado não supervisionado que identifica e trata outliers com base em padrões aprendidos nos dados.\n",
    "\n",
    "- **Vantagens**: Eficaz para grandes conjuntos de dados e alta dimensionalidade e não precisa de distribuição normal dos dados\n",
    "- **Desvantagens**: Pode ser complexo de implementar e parâmetros podem ser difíceis de ajustar.\n",
    "\n",
    "> **Detecção de Outliers com Local Outlier Factor (LOF)**: Algoritmo de aprendizado não supervisionado que identifica outliers com base na densidade local dos dados.\n",
    "\n",
    "- **Vantagens**: Detecta outliers em regiões de diferentes densidades.\n",
    "- **Desvantagens**: Pode ser computacionalmente intensivo e sensível ao número de vizinhos (parâmetro k).\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Clusterização**: Utiliza algoritmos de clusterização para identificar e tratar outliers como pontos em clusters diferentes ou distantes.\n",
    "\n",
    "- **Vantagens**: Pode identificar outliers como pontos em clusters menores ou distantes e é adaptável a diferentes formas de dados.\n",
    "- **Desvantagens**: Requer escolha de número de clusters e computacionalmente caro.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tratando Valores Outliers\n",
    "\n",
    "# Função para aplicar Winsorização\n",
    "def apply_winsorization(X, limits):\n",
    "    return X.apply(lambda col: winsorize(col, limits=limits))\n",
    "\n",
    "# Limites para Winsorização\n",
    "winsor_limits = (0.05, 0.05)\n",
    "\n",
    "# Obter as colunas de X_treino exceto a(s) indicada(s)\n",
    "variables_to_winsorize = X_treino.columns.difference(['idade_do_canal'])\n",
    "\n",
    "# Winsorização das variáveis numéricas contínuas nos dados de treino e teste\n",
    "X_treino[variables_to_winsorize] = apply_winsorization(X_treino[variables_to_winsorize], winsor_limits)\n",
    "X_teste[variables_to_winsorize] = apply_winsorization(X_teste[variables_to_winsorize], winsor_limits)\n",
    "\n",
    "# Winsorização da variável alvo de treino e teste\n",
    "y_treino = winsorize(y_treino, winsor_limits)\n",
    "y_teste = winsorize(y_teste, winsor_limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc5e54",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tratamento de Escala\n",
    "\n",
    "> **Normalização** ou **Padronização**\n",
    "\n",
    "- A transformação (como escalonamento) precisa ser aplicada nos dados de teste usando os parâmetros calculados a partir dos dados de treino para garantir a consistência. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Padronizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fad567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031, 14) (258, 14) (1031,) (258,)\n"
     ]
    }
   ],
   "source": [
    "## Padronização\n",
    "\n",
    "# Separar as colunas que não serão padronizadas\n",
    "non_scaled_columns = ['idade_do_canal']\n",
    "X_treino_non_scaled = X_treino[non_scaled_columns]\n",
    "X_teste_non_scaled = X_teste[non_scaled_columns]\n",
    "\n",
    "X_treino_to_scale = X_treino.drop(columns=non_scaled_columns)\n",
    "X_teste_to_scale = X_teste.drop(columns=non_scaled_columns)\n",
    "\n",
    "# Padronização dos dados de treino e teste\n",
    "scaler_X = StandardScaler()\n",
    "X_treino_scaled = scaler_X.fit_transform(X_treino_to_scale)\n",
    "X_teste_scaled = scaler_X.transform(X_teste_to_scale)\n",
    "\n",
    "# Reconstruir os DataFrames com as colunas não padronizadas\n",
    "X_treino_scaled = pd.DataFrame(X_treino_scaled, columns=X_treino_to_scale.columns, index=X_treino.index)\n",
    "X_teste_scaled = pd.DataFrame(X_teste_scaled, columns=X_teste_to_scale.columns, index=X_teste.index)\n",
    "\n",
    "# Manter a coluna 'idade_do_canal' na mesma posição\n",
    "X_treino = pd.concat([X_treino_scaled, X_treino_non_scaled], axis=1)\n",
    "X_treino = X_treino[X.columns]\n",
    "X_teste = pd.concat([X_teste_scaled, X_teste_non_scaled], axis=1)\n",
    "X_teste = X_teste[X.columns]\n",
    "\n",
    "# Padronização da variável alvo de treino e teste\n",
    "scaler_y = StandardScaler()\n",
    "y_treino_scaled = scaler_y.fit_transform(y_treino.reshape(-1, 1))\n",
    "y_teste_scaled = scaler_y.transform(y_teste.reshape(-1, 1))\n",
    "\n",
    "# Suponha que este é o DataFrame de treino original antes do ajuste\n",
    "X_treino_columns = X_treino.columns\n",
    "print(\"Colunas após tratamento:\", X_treino_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b96cfa",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "\n",
    "<span style=\"color: green; font-size: 40px; font-weight: bold;\">Construindo Modelos de Machine Learning</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "- Nesta etapa do projeto o ideal é escolher um algoritmo simples e fácil de compreender, que será usado como Benchmark (modelo base).\n",
    "\n",
    "#### Importante\n",
    "\n",
    "- Iremos treinar dois conjuntos de dados: um **conjunto de dados com todas as variáveis** e outro **variáveis selecionadas**.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "## Criando Dataframe para salvar métricas de cada Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688031e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dataframe para receber as métricas de cada modelo\n",
    "df_modelos = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49d79c",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "# <span style=\"color: green; font-weight: bold;\">Modelo 1 com Regressão Logística (Benchmark)</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "> # Versão 1\n",
    "\n",
    "- Sem Ajuste de Hiperparâmetros\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Criação, Treinamento, Previsão e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4faee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03066409",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Salvando as Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70be005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um DataFrame para salvar as métricas\n",
    "modelo_v1 = pd.DataFrame({\n",
    "    'Nome do Modelo': ['Logistic Regression\t'],\n",
    "    'Versao': ['1'],\n",
    "    'Tipo de Dados': ['Todas as Variáveis'],\n",
    "    'Tipo de Modelo': ['Sem Ajuste de Hiperparâmetros'],\n",
    "    'MAE': [f\"{mae:.2f}\"],\n",
    "    'MSE': [f\"{mse:.2f}\"],\n",
    "    'RMSE': [rmse],\n",
    "    'Coeficiente R2': [r2],\n",
    "    'Variância Explicada': [evs]\n",
    "})\n",
    "\n",
    "# Concatenando com o DataFrame existente\n",
    "df_modelos = pd.concat([df_modelos, modelo_v1], ignore_index=True)\n",
    "\n",
    "# Visualizando DataFrame\n",
    "display(df_modelos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80626863",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "> # Versão 2\n",
    "\n",
    "- Com Ajuste de Hiperparâmetros\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### Configurando Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770dddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5848f10",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Criação, Treinamento, Previsão e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13f9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3338c852",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Salvando as Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9801b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed20321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "167eb3fd-97f9-4b05-8fcf-90004a30223e",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# SELECIONANDO O MELHOR MODELO\n",
    "\n",
    "- Usaremos o modelo que .\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Visualizando Dataframe Ordenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenando o DataFrame pelo (modificar sort_values)\n",
    "df_modelos_sorted = df_modelos.sort_values(by='AUC Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Visualizando Daframe\n",
    "display(df_modelos_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff553dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "632f8a4d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Salvando e Carregando Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366b780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d0e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af9c9c2e",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Salvando Pipeline com Melhor Modelo e Escalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a72ad15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nome do Modelo                     XGBoost Regressor\n",
       "Versao                                             2\n",
       "Tipo de Dados                              Reduzidos\n",
       "Tipo de Modelo         Com Ajuste de Hiperparâmetros\n",
       "MAE                                       686.908511\n",
       "MSE                                   8098442.845715\n",
       "RMSE                                     2845.776317\n",
       "Coeficiente R2                              0.994235\n",
       "Variância Explicada                         0.994277\n",
       "Name: 11, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos salvos com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Selecionar o melhor modelo com base no RMSE\n",
    "melhor_modelo = df_modelos.nsmallest(1, 'RMSE').iloc[0]\n",
    "\n",
    "# Visualizando dados do melhor modelo\n",
    "display(melhor_modelo)\n",
    "\n",
    "# Salvar os parâmetros das transformações e o melhor modelo\n",
    "joblib.dump(scaler_X, 'pipeline_projeto_youtube/scaler_X.pkl')\n",
    "joblib.dump(scaler_y, 'pipeline_projeto_youtube/scaler_y.pkl')\n",
    "joblib.dump(modelo_GBR_v2, 'pipeline_projeto_youtube/modelo_XGB_v2.pkl')\n",
    "joblib.dump(winsor_limits, 'pipeline_projeto_youtube/winsor_limits.pkl')\n",
    "\n",
    "joblib.dump(label_encoders, 'pipeline_projeto_youtube/label_encoders.pkl') # Salvar todos os LabelEncoders\n",
    "\n",
    "joblib.dump(X_treino_columns, 'pipeline_projeto_youtube/X_treino_columns.pkl') # Salvar os nomes das colunas\n",
    "\n",
    "\n",
    "print('Arquivos salvos com sucesso.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83696d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Carregando Modelo e Pipeline Salvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd59ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c82bd615",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Previsões com Novos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03a521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f7aecf5",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# <span style=\"color: green; font-size: 38px; font-weight: bold;\">CRIANDO INTERFACE</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "# Versão 1\n",
    "\n",
    "- Usando **Jupyter Widgets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442345cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90173baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d3ba470",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "====================================================================================================================\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# Versão 2\n",
    "\n",
    "- Versão mais elaborada usando o **Streamlit**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff99aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a664a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b4627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7ad6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1678944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0f4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd0611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cfc96d9",
   "metadata": {},
   "source": [
    "<table border=\"2\" style=\"font-size: 14px; border-spacing: 10px;\">\n",
    "  <caption style=\"font-size: 32px; margin: 30px; text-align: center\">Pipeline para Modelos de Machine Learning (ML)</caption>\n",
    "  <tr>\n",
    "    <th style=\"text-align: center; font-size: 20px;\">Passo</th>\n",
    "    <th style=\"text-align: center; font-size: 20px;\">Descrição</th>\n",
    "    <th style=\"text-align: center; font-size: 20px;\">Comentário</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Definir Objetivo e Pergunta de Negócio</td>\n",
    "    <td>Determinar o objetivo do projeto e as principais perguntas de negócio a serem respondidas.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Importar Pacotes</td>\n",
    "    <td>Importar as bibliotecas e pacotes necessários para o projeto.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Carregar dados</td>\n",
    "    <td>Carregar os dados é a primeira etapa essencial para qualquer projeto de Data Science.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>Analisar Dados de forma Geral</td>\n",
    "    <td>Realizar uma análise exploratória inicial para entender a estrutura e o conteúdo dos dados.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>Remover Variáveis Completamente Irrelevantes</td>\n",
    "    <td>Remover variáveis que não contribuem para a análise ou que são completamente irrelevantes.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>Renomear Variáveis (se necessário)</td>\n",
    "    <td>Renomear as variáveis para melhor visualização e entendimento dos dados.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>Analisar Variáveis Categóricas</td>\n",
    "    <td>Entender a distribuição e as características das variáveis categóricas.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>Aplicar Tratamento nas Variáveis Categóricas antes do Label Encode (se necessário)</td>\n",
    "    <td>Tratamentos como combinação de categorias raras ou correção de inconsistências devem ser feitos antes da codificação.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>Aplicar Label Encode nas Variáveis Categóricas</td>\n",
    "    <td>Converter variáveis categóricas em numéricas.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>Analisar Todas as Variáveis com Describe e Gráficos</td>\n",
    "    <td>Realizar uma análise estatística e visual de todas as variáveis para identificar padrões e anomalias.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>Tratar Valores Ausentes</td>\n",
    "    <td>Tratar valores ausentes antes da modelagem para evitar problemas.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>Tratar Linhas Duplicadas</td>\n",
    "    <td>Remover ou lidar com linhas duplicadas para garantir a qualidade dos dados.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>Tratar Valores Outliers</td>\n",
    "    <td>Identificar e tratar outliers que possam afetar a performance do modelo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>14</td>\n",
    "    <td>Feature Engineering (se necessário)</td>\n",
    "    <td>Criar novas variáveis ou transformar variáveis existentes para melhorar a capacidade preditiva do modelo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>15</td>\n",
    "    <td>Aplicar Clusterização (se necessário)</td>\n",
    "    <td>Identificar padrões ou segmentos nos dados para insights adicionais.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>16</td>\n",
    "    <td>Análise de Correlação</td>\n",
    "    <td>Identificar relações entre as variáveis que podem informar a seleção de variáveis.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>17</td>\n",
    "    <td>Seleção de Variáveis</td>\n",
    "    <td>Selecionar as variáveis mais relevantes para a modelagem.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>18</td>\n",
    "    <td>Remover Variáveis Após Análise de Correlação e Seleção de Variáveis (se necessário)</td>\n",
    "    <td>Remover variáveis que não são relevantes com base nas análises de correlação e seleção de variáveis.</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td>19</td>\n",
    "    <td>Dividir os Dados em Treino e Teste</td>\n",
    "    <td>Separar os dados para validar o desempenho do modelo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>Tratar Outliers que Envolvam Mudança de Escala</td>\n",
    "    <td>Aplicar tratamentos como winsorização e mudanças de escala.</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td>21</td>\n",
    "    <td>Aplicar Normalização ou Padronização (Depende dos Dados)</td>\n",
    "    <td>Normalizar ou padronizar os dados após a divisão em treino e teste para evitar vazamento de dados.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>Criar Modelo Benchmark</td>\n",
    "    <td>Criar um modelo inicial simples para ter uma linha de base de desempenho.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>23</td>\n",
    "    <td>Melhorar o Conjunto de Dados (se necessário)</td>\n",
    "    <td>Melhorar os dados, se necessário, para tentar aumentar o desempenho do modelo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>24</td>\n",
    "    <td>Treinar Outros Algoritmos</td>\n",
    "    <td>Testar múltiplos algoritmos para encontrar o mais adequado.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>25</td>\n",
    "    <td>Verificar Desempenho dos Algoritmos</td>\n",
    "    <td>Comparar os desempenhos para selecionar o melhor modelo.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>26</td>\n",
    "    <td>Escolher o Melhor Modelo</td>\n",
    "    <td>Selecionar o modelo com melhor desempenho.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>27</td>\n",
    "    <td>Salvar Pipeline com Informações sobre o Projeto</td>\n",
    "    <td>Selecionar o modelo com melhor desempenho e salvá-lo para uso futuro, incluindo no pipeline as informações necessárias para desnormalizar ou despadronizar os dados, bem como os labels e outras configurações.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>28</td>\n",
    "    <td>Testar o Modelo com Dados Novos</td>\n",
    "    <td>Validar o modelo com novos dados para verificar sua capacidade de generalização.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>29</td>\n",
    "    <td>Criar uma Interface Gráfica do Modelo</td>\n",
    "    <td>Criar uma interface gráfica pode ser útil, mas não é um passo essencial em todos os projetos de Data Science.</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0791e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Padronização x Normalização\n",
    "\n",
    "As técnicas de padronização e normalização são usadas no pré-processamento de dados em aprendizado de máquina para preparar variáveis numéricas, ajustando suas escalas. Aqui está quando e por que usar cada uma:\n",
    "\n",
    "<br>\n",
    "\n",
    "### Padronização\n",
    "Transforma os dados de modo que eles tenham média zero e desvio padrão igual a um. \n",
    "- **Quando usar**: Aplicável quando os dados já estão centralizados em torno de uma média e precisam de ajuste na escala. É útil em modelos como SVM e Regressão Logística, que são sensíveis a variações na escala das variáveis de entrada.\n",
    "- **Exemplo prático**: Se medimos altura em centímetros (150-190 cm) e peso em quilogramas (50-100 kg), a padronização permite comparar essas medidas numa escala comum, evitando distorções devido a diferentes intervalos de valores.\n",
    "- **Por que escolher para este projeto**: Optamos pela padronização porque as variáveis têm escalas muito diferentes e há a presença de outliers significativos. A padronização mantém as propriedades estatísticas dos dados, minimizando o impacto dos outliers, ao contrário da normalização que pode distorcer os dados ao comprimir a maioria dos valores em um intervalo estreito.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Normalização\n",
    "Ajusta os dados para que seus valores caibam em um intervalo predefinido, geralmente de **0 a 1**.\n",
    "- **Quando usar**: Ideal para dados com variações extremas nas escalas e onde os algoritmos são sensíveis à magnitude absoluta dos dados, como K-Nearest Neighbors (KNN) e técnicas de clustering.\n",
    "- **Exemplo prático**: Se um dataset contém preços de produtos variando de R$1 a R$1000 e quantidades vendidas de 1 a 20 unidades, a normalização faria com que ambos os atributos tivessem a mesma contribuição no modelo, independentemente da escala original.\n",
    "- **Por que não usamos aqui**: Não foi escolhida devido à presença de outliers, que poderiam ser enfatizados indevidamente, e porque a normalização poderia limitar a eficácia de modelos que assumem uma distribuição normal dos dados.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Importante:\n",
    "- **Não é necessário** aplicar padronização/normalização na **variável alvo**.\n",
    "- Nós **não aplicamos** as duas técnicas, ou usamos uma ou outra.\n",
    "- A **normalização** pode não ser a melhor escolha se houver **outliers significativos no conjunto de dados**, pois isso poderia comprimir a maioria dos dados em um intervalo muito estreito. Nesses casos, a **padronização é recomendada**.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5eedd",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "# Análise Temporal\n",
    "\n",
    "## Informações Necessárias para Análise Temporal\n",
    "\n",
    "### Para fazer uma análise temporal, é obrigatório ter:\n",
    "\n",
    "- **Variável Alvo (Target Variable)**: A variável que você deseja prever, como ganhos_mensais_estimados_minimo.\n",
    "- **Marca Temporal (Timestamp)**: Uma coluna que representa o tempo em que cada observação foi feita. Isso pode ser uma data, hora ou uma combinação de ambas. Pode ser derivado de novas colunas criadas a partir de uma coluna de Data como dia_criacao_do_canal, mes_criacao_do_canal e ano_criacao_do_canal.\n",
    "\n",
    "### Qual formato os Dados devem estar ?\n",
    "\n",
    "Os dados devem estar no formato adequado:\n",
    "\n",
    "- **Marca Temporal (Timestamp)**: Deve estar no tipo de dados datetime. No pandas, você pode usar pd.to_datetime para converter uma coluna para o tipo datetime.\n",
    "- **Variável Alvo e Outras Variáveis**: Normalmente devem ser numéricas (int, float).\n",
    "\n",
    "### Divisão do Conjunto de Dados em Treino e Teste\n",
    "\n",
    "> É uma **boa prática** dividir os dados em conjuntos de treino e teste, mesmo para análises temporais. Isso permite avaliar o desempenho do modelo de previsão. No entanto, a divisão deve respeitar a sequência temporal:\n",
    "\n",
    "- **Treino**: Usar dados históricos até um certo ponto no tempo.\n",
    "- **Teste**: Usar dados subsequentes para avaliar a previsão.\n",
    "\n",
    "### Avaliação do Modelo\n",
    "\n",
    "> Existem várias métricas para avaliar modelos de séries temporais:\n",
    "\n",
    "- **MAE (Mean Absolute Error)**: Média dos erros absolutos.\n",
    "- **MSE (Mean Squared Error)**: Média dos erros quadrados.\n",
    "- **RMSE (Root Mean Squared Error)**: Raiz quadrada da média dos erros quadrados.\n",
    "- **MAPE (Mean Absolute Percentage Error)**: Média dos erros absolutos percentuais.\n",
    "\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c409719",
   "metadata": {},
   "source": [
    "# Exemplo de Preparação de Dados e Criação de Modelo com Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe589412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>549.671415</td>\n",
       "      <td>1639.935544</td>\n",
       "      <td>4895.126419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>486.173570</td>\n",
       "      <td>1592.463368</td>\n",
       "      <td>4636.221513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>564.768854</td>\n",
       "      <td>1505.963037</td>\n",
       "      <td>4666.611639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>652.302986</td>\n",
       "      <td>1435.306322</td>\n",
       "      <td>4812.123525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476.584663</td>\n",
       "      <td>1569.822331</td>\n",
       "      <td>4474.717917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1     feature2       target\n",
       "0  549.671415  1639.935544  4895.126419\n",
       "1  486.173570  1592.463368  4636.221513\n",
       "2  564.768854  1505.963037  4666.611639\n",
       "3  652.302986  1435.306322  4812.123525\n",
       "4  476.584663  1569.822331  4474.717917"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Exemplo de dados\n",
    "np.random.seed(42)  # Para reprodutibilidade\n",
    "\n",
    "# Gerar dados\n",
    "N = 1000\n",
    "feature1 = np.random.randn(N) * 100 + 500\n",
    "feature2 = np.random.randn(N) * 100 + 1500\n",
    "target = 3 * feature1 + 2 * feature2 + np.random.randn(N) * 50  # Combinação linear com ruído\n",
    "\n",
    "# Criar DataFrame\n",
    "dados = pd.DataFrame({\n",
    "    'feature1': feature1,\n",
    "    'feature2': feature2,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "# Exibir os primeiros dados\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9920a4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 2) (200, 2) (800,) (200,)\n"
     ]
    }
   ],
   "source": [
    "## Separar os dados em treino e teste\n",
    "\n",
    "# Cria um objeto separado para a variável alvo\n",
    "y = dados['target']\n",
    "\n",
    "# Cria um objeto separado para as variáveis de entrada\n",
    "X = dados.drop('target', axis=1)\n",
    "\n",
    "# Split em dados de treino e teste sem amostragem estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print do shape\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aeb16fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tratando Valores Outliers\n",
    "\n",
    "# Função para aplicar Winsorização\n",
    "def apply_winsorization(X, limits):\n",
    "    return X.apply(lambda col: winsorize(col, limits=limits))\n",
    "\n",
    "winsor_limits = (0.05, 0.05)\n",
    "\n",
    "# Winsorização dos dados de treino e teste\n",
    "X_train_winsor = apply_winsorization(X_train, winsor_limits)\n",
    "X_test_winsor = apply_winsorization(X_test, winsor_limits)\n",
    "\n",
    "# Winsorização da variável alvo de treino e teste\n",
    "y_train_winsor = winsorize(y_train, winsor_limits)\n",
    "y_test_winsor = winsorize(y_test, winsor_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d30f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padronização\n",
    "\n",
    "# Padronização dos dados de treino e teste\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_winsor)\n",
    "X_test_scaled = scaler_X.transform(X_test_winsor)\n",
    "\n",
    "# Padronização da variável alvo de treino e teste\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_winsor.reshape(-1, 1))\n",
    "y_test_scaled = scaler_y.transform(y_test_winsor.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9af87bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE - Erro Médio Absoluto: 49.73811870313032\n",
      "MSE - Erro Quadrático Médio: 4558.624461869952\n",
      "RMSE - Raiz Quadrada do Erro Quadrático Médio: 67.5175863154923\n",
      "Coeficiente R2: 0.9573789613443023\n",
      "Variância Explicada: 0.9579785922314106\n"
     ]
    }
   ],
   "source": [
    "## Treinando o Modelo\n",
    "\n",
    "# Treinar o modelo de regressão linear\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "## Previsões\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "\n",
    "# Reverter a padronização das previsões e dos valores reais para a escala original\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_original = scaler_y.inverse_transform(y_test_scaled)\n",
    "\n",
    "\n",
    "## Avaliação\n",
    "\n",
    "# Calcular métricas de avaliação\n",
    "r2 = r2_score(y_test_original, y_pred)\n",
    "mae = mean_absolute_error(y_test_original, y_pred)\n",
    "mse = mean_squared_error(y_test_original, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred))\n",
    "evs = explained_variance_score(y_test_original, y_pred)\n",
    "\n",
    "print('MAE - Erro Médio Absoluto:', mae)\n",
    "print('MSE - Erro Quadrático Médio:', mse)\n",
    "print('RMSE - Raiz Quadrada do Erro Quadrático Médio:', rmse)\n",
    "print('Coeficiente R2:', r2)\n",
    "print('Variância Explicada:', evs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ba2a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tudo salvo.\n"
     ]
    }
   ],
   "source": [
    "# Salvar os parâmetros das transformações e o modelo\n",
    "joblib.dump(scaler_X, 'pipeline/scaler_X.pkl')\n",
    "joblib.dump(scaler_y, 'pipeline/scaler_y.pkl')\n",
    "joblib.dump(model, 'pipeline/model.pkl')\n",
    "joblib.dump(winsor_limits, 'pipeline/winsor_limits.pkl')\n",
    "\n",
    "print('tudo salvo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e6fbe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previsões (em escala original): [[4928.86152035]]\n"
     ]
    }
   ],
   "source": [
    "# Carregar os parâmetros salvos e o modelo para novos dados\n",
    "scaler_X = joblib.load('pipeline/scaler_X.pkl')\n",
    "scaler_y = joblib.load('pipeline/scaler_y.pkl')\n",
    "model = joblib.load('pipeline/model.pkl')\n",
    "winsor_limits = joblib.load('pipeline/winsor_limits.pkl')\n",
    "\n",
    "# Exemplo de novos dados\n",
    "new_data = pd.DataFrame({\n",
    "    'feature1': [549.671415],\n",
    "    'feature2': [1639.935544]\n",
    "})\n",
    "\n",
    "# Aplicar as mesmas transformações aos novos dados (Winsorização e Padronização)\n",
    "new_data_winsor = apply_winsorization(new_data, winsor_limits)\n",
    "new_data_scaled = scaler_X.transform(new_data_winsor)\n",
    "\n",
    "# Fazer previsões nos novos dados\n",
    "predictions_scaled = model.predict(new_data_scaled)\n",
    "\n",
    "# Reverter a padronização das previsões para a escala original\n",
    "predictions_original_scale = scaler_y.inverse_transform(predictions_scaled)\n",
    "\n",
    "print(\"Previsões (em escala original):\", predictions_original_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ed13f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Exemplo de Preparação de Dados e Criação de Modelo com Classificação Multiclasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "116980ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>524.835708</td>\n",
       "      <td>1548.755987</td>\n",
       "      <td>B</td>\n",
       "      <td>Class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>493.086785</td>\n",
       "      <td>1492.647131</td>\n",
       "      <td>C</td>\n",
       "      <td>Class1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1     feature2 feature3  target\n",
       "0  524.835708  1548.755987        B  Class1\n",
       "1  493.086785  1492.647131        C  Class1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>524.835708</td>\n",
       "      <td>1548.755987</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>493.086785</td>\n",
       "      <td>1492.647131</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1     feature2  feature3  target\n",
       "0  524.835708  1548.755987         1       0\n",
       "1  493.086785  1492.647131         2       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Exemplo de dados\n",
    "np.random.seed(42)  # Para reprodutibilidade\n",
    "\n",
    "# Gerar dados\n",
    "N = 1000\n",
    "\n",
    "# Dividir N em três partes iguais\n",
    "N_per_class = N // 3\n",
    "\n",
    "feature1_class1 = np.random.randn(N_per_class + 1) * 50 + 500\n",
    "feature2_class1 = np.random.randn(N_per_class + 1) * 50 + 1500\n",
    "feature1_class2 = np.random.randn(N_per_class) * 50 + 700\n",
    "feature2_class2 = np.random.randn(N_per_class) * 50 + 1300\n",
    "feature1_class3 = np.random.randn(N_per_class) * 50 + 900\n",
    "feature2_class3 = np.random.randn(N_per_class) * 50 + 1100\n",
    "\n",
    "feature1 = np.concatenate([feature1_class1, feature1_class2, feature1_class3])\n",
    "feature2 = np.concatenate([feature2_class1, feature2_class2, feature2_class3])\n",
    "feature3 = np.random.choice(['A', 'B', 'C'], size=N)\n",
    "\n",
    "target = np.array(['Class1'] * N_per_class + ['Class2'] * N_per_class + ['Class3'] * N_per_class)\n",
    "target = np.append(target, 'Class3')\n",
    "\n",
    "# Verificar se todas as arrays têm o mesmo comprimento\n",
    "print(len(feature1), len(feature2), len(feature3), len(target))\n",
    "\n",
    "# Criar DataFrame\n",
    "dados = pd.DataFrame({\n",
    "    'feature1': feature1,\n",
    "    'feature2': feature2,\n",
    "    'feature3': feature3,\n",
    "    'target': target\n",
    "})\n",
    "\n",
    "display(dados.head(2))\n",
    "\n",
    "\n",
    "# Codificar variáveis categóricas\n",
    "label_encoder_features = LabelEncoder()\n",
    "dados['feature3'] = label_encoder_features.fit_transform(dados['feature3'])\n",
    "\n",
    "# Codificar a variável alvo\n",
    "label_encoder_target = LabelEncoder()\n",
    "dados['target'] = label_encoder_target.fit_transform(dados['target'])\n",
    "\n",
    "display(dados.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bc428504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 3) (200, 3) (800,) (200,)\n"
     ]
    }
   ],
   "source": [
    "## Separar os dados em treino e teste\n",
    "\n",
    "# Cria um objeto separado para a variável alvo\n",
    "y = dados['target']\n",
    "\n",
    "# Cria um objeto separado para as variáveis de entrada\n",
    "X = dados.drop('target', axis=1)\n",
    "\n",
    "# Split em dados de treino e teste sem amostragem estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print do shape\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5fb7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tratando Valores Outliers\n",
    "\n",
    "# Função para aplicar Winsorização\n",
    "def apply_winsorization(X, limits):\n",
    "    return X.apply(lambda col: winsorize(col, limits=limits))\n",
    "\n",
    "winsor_limits = (0.05, 0.05)\n",
    "\n",
    "# Winsorização das variáveis numéricas contínuas nos dados de treino e teste\n",
    "X_train[['feature1', 'feature2']] = apply_winsorization(X_train[['feature1', 'feature2']], winsor_limits)\n",
    "X_test[['feature1', 'feature2']] = apply_winsorization(X_test[['feature1', 'feature2']], winsor_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d339d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padronização\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Padronização das variáveis numéricas contínuas nos dados de treino e teste\n",
    "X_train[['feature1', 'feature2']] = scaler.fit_transform(X_train[['feature1', 'feature2']])\n",
    "X_test[['feature1', 'feature2']] = scaler.transform(X_test[['feature1', 'feature2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa794300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.9850\n",
      "Precisão: 0.9857\n",
      "Recall: 0.9850\n",
      "F1-Score: 0.9851\n"
     ]
    }
   ],
   "source": [
    "## Treinar o Modelo\n",
    "\n",
    "# Treinar o modelo de classificação\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "## Previsões\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de avaliação\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Acurácia: {accuracy:.4f}\")\n",
    "print(f\"Precisão: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6825975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tudo salvo.\n"
     ]
    }
   ],
   "source": [
    "# Salvar os parâmetros das transformações e o modelo\n",
    "joblib.dump(scaler, 'pipeline/scaler.pkl')\n",
    "joblib.dump(label_encoder_features, 'pipeline/label_encoder_features.pkl')\n",
    "joblib.dump(label_encoder_target, 'pipeline/label_encoder_target.pkl')\n",
    "joblib.dump(model, 'pipeline/model.pkl')\n",
    "joblib.dump(winsor_limits, 'pipeline/winsor_limits.pkl')\n",
    "\n",
    "print('tudo salvo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7778a845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>524.835708</td>\n",
       "      <td>1548.755987</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1     feature2 feature3\n",
       "0   524.835708  1548.755987        B\n",
       "1  1500.000000   800.000000        C"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previsões (classe original): ['Class1' 'Class3']\n"
     ]
    }
   ],
   "source": [
    "# Carregar os parâmetros salvos e o modelo para novos dados\n",
    "scaler = joblib.load('pipeline/scaler.pkl')\n",
    "label_encoder_features = joblib.load('pipeline/label_encoder_features.pkl')\n",
    "label_encoder_target = joblib.load('pipeline/label_encoder_target.pkl')\n",
    "model = joblib.load('pipeline/model.pkl')\n",
    "winsor_limits = joblib.load('pipeline/winsor_limits.pkl')\n",
    "\n",
    "# Exemplo de novos dados\n",
    "new_data = pd.DataFrame({\n",
    "    'feature1': [524.835708, 1500],\n",
    "    'feature2': [1548.755987, 800],\n",
    "    'feature3': ['B', 'C']\n",
    "})\n",
    "\n",
    "display(new_data)\n",
    "\n",
    "# Codificar variáveis categóricas nos novos dados\n",
    "new_data['feature3'] = label_encoder_features.transform(new_data['feature3'])\n",
    "\n",
    "# Aplicar as mesmas transformações aos novos dados (Winsorização e Padronização)\n",
    "new_data[['feature1', 'feature2']] = apply_winsorization(new_data[['feature1', 'feature2']], winsor_limits)\n",
    "new_data[['feature1', 'feature2']] = scaler.transform(new_data[['feature1', 'feature2']])\n",
    "\n",
    "# Fazer previsões nos novos dados\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "# Reverter a codificação das previsões para as classes originais\n",
    "predictions_original = label_encoder_target.inverse_transform(predictions)\n",
    "\n",
    "print(\"Previsões (classe original):\", predictions_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f719ece",
   "metadata": {},
   "source": [
    "<br> <br> <br>\n",
    "\n",
    "# O que é VectorAssembler no Apache Spark?\n",
    "\n",
    "### VectorAssembler\n",
    "\n",
    "- A função VectorAssembler no Apache Spark é um transformador que combina uma lista de colunas em uma única coluna de vetor. Este vetor de atributos é utilizado como entrada para treinar modelos de Machine Learning. O VectorAssembler é especialmente útil quando precisamos combinar várias características (features) de entrada em uma única representação vetorial, que é o formato esperado por muitos algoritmos de Machine Learning.\n",
    "\n",
    "#### Tipos de Colunas Aceitas\n",
    "\n",
    "O VectorAssembler aceita os seguintes tipos de colunas de entrada:\n",
    "\n",
    "- Todos os tipos numéricos (inteiros, flutuantes, etc.)\n",
    "- Tipo booleano\n",
    "- Tipo vetorial (como DenseVector ou SparseVector)\n",
    "\n",
    "#### Funcionamento\n",
    "\n",
    "- Em cada linha do DataFrame, os valores das colunas de entrada especificadas são concatenados em um vetor na ordem que você define. Este vetor de características (features) será então usado como variável de entrada para treinar o modelo de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fee48d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Este pipeline completo demonstra como incluir a padronização após a codificação das variáveis categóricas e a combinação das características em um vetor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3a3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 17:16:51 WARN Utils: Your hostname, eduardo-Inspiron-15-3520 resolves to a loopback address: 127.0.1.1; using 192.168.0.13 instead (on interface wlp0s20f3)\n",
      "24/08/06 17:16:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/06 17:16:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/06 17:16:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xf. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+-----+\n",
      "| id|category1|category2|value|\n",
      "+---+---------+---------+-----+\n",
      "|  0|        a|        x| 10.0|\n",
      "|  1|        b|        y| 20.0|\n",
      "|  2|        c|        z| 30.0|\n",
      "|  3|        a|        x| 40.0|\n",
      "|  4|        a|        y| 50.0|\n",
      "|  5|        c|        z| 60.0|\n",
      "+---+---------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+-----+--------------------+\n",
      "| id|category1|category2|value|     scaled_features|\n",
      "+---+---------+---------+-----+--------------------+\n",
      "|  0|        a|        x| 10.0|[1.82574185835055...|\n",
      "|  1|        b|        y| 20.0|(5,[3,4],[1.93649...|\n",
      "|  2|        c|        z| 30.0|(5,[1,4],[1.93649...|\n",
      "|  3|        a|        x| 40.0|[1.82574185835055...|\n",
      "|  4|        a|        y| 50.0|[1.82574185835055...|\n",
      "|  5|        c|        z| 60.0|(5,[1,4],[1.93649...|\n",
      "+---+---------+---------+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 36406)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/eduardo/anaconda3/lib/python3.9/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Inicializar SparkSession\n",
    "spark = SparkSession.builder.appName(\"CategoricalEncoding\").getOrCreate()\n",
    "\n",
    "# Exemplo de dados\n",
    "data = [\n",
    "    (0, \"a\", \"x\", 10.0),\n",
    "    (1, \"b\", \"y\", 20.0),\n",
    "    (2, \"c\", \"z\", 30.0),\n",
    "    (3, \"a\", \"x\", 40.0),\n",
    "    (4, \"a\", \"y\", 50.0),\n",
    "    (5, \"c\", \"z\", 60.0)\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "columns = [\"id\", \"category1\", \"category2\", \"value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mostrar DataFrame\n",
    "df.show()\n",
    "\n",
    "# StringIndexer para category1 e category2\n",
    "indexer1 = StringIndexer(inputCol=\"category1\", outputCol=\"category1_index\")\n",
    "indexer2 = StringIndexer(inputCol=\"category2\", outputCol=\"category2_index\")\n",
    "\n",
    "# OneHotEncoder para category1_index e category2_index\n",
    "encoder1 = OneHotEncoder(inputCol=\"category1_index\", outputCol=\"category1_encoded\")\n",
    "encoder2 = OneHotEncoder(inputCol=\"category2_index\", outputCol=\"category2_encoded\")\n",
    "\n",
    "# VectorAssembler para combinar todas as colunas de recursos em uma única coluna de vetor\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"category1_encoded\", \"category2_encoded\", \"value\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# StandardScaler para padronizar as características\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[indexer1, indexer2, encoder1, encoder2, assembler, scaler])\n",
    "\n",
    "# Ajustar e transformar os dados\n",
    "pipeline_model = pipeline.fit(df)\n",
    "df_transformed = pipeline_model.transform(df)\n",
    "\n",
    "# Mostrar DataFrame transformado\n",
    "df_transformed.select(\"id\", \"category1\", \"category2\", \"value\", \"scaled_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f812e63",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "# O que é Vetores Densos e Vetores Esparsos em Objetos RDD?\n",
    "\n",
    "### Vetores Densos\n",
    "\n",
    "#### Descrição:\n",
    "- Vetores densos são vetores em que todos os elementos são armazenados explicitamente, independentemente de serem zero ou não. Eles são representados como uma lista ou um array que contém todos os valores dos elementos na mesma ordem em que aparecem.\n",
    "\n",
    "#### Uso:\n",
    "- Os vetores densos são mais eficientes e apropriados quando a maioria dos valores são diferentes de zero. Isso porque o armazenamento explícito de todos os elementos não desperdiça muito espaço, já que há poucos zeros. São ideais em situações onde os dados são completos e não esparsos.\n",
    "\n",
    "#### Exemplo em RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b1f418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, DenseVector([1.0, 2.0, 3.0])), (0, DenseVector([4.0, 5.0, 6.0])), (1, DenseVector([7.0, 8.0, 9.0]))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Inicializar SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Exemplo de dados\n",
    "data = [\n",
    "    (1, Vectors.dense([1.0, 2.0, 3.0])),\n",
    "    (0, Vectors.dense([4.0, 5.0, 6.0])),\n",
    "    (1, Vectors.dense([7.0, 8.0, 9.0]))\n",
    "]\n",
    "\n",
    "# Criar RDD\n",
    "rdd = sc.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0e848",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Vetores Esparsos\n",
    "\n",
    "#### Descrição:\n",
    "- Vetores esparsos são vetores em que apenas os elementos não-zero são armazenados, junto com suas posições. Eles são representados de forma compacta, especificando o tamanho do vetor, uma lista de índices de elementos não-zero, e os valores correspondentes a esses índices.\n",
    "\n",
    "### Uso:\n",
    "- Os vetores esparsos são ideais quando a maioria dos valores no vetor são zero, economizando espaço e tempo de computação. Eles são especialmente úteis em problemas de Machine Learning e Data Mining, onde os dados são frequentemente esparsos, como em representações de texto (bag-of-words) e matrizes de incidência.\n",
    "\n",
    "#### Exemplo em RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6291c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, SparseVector(5, {0: 1.0, 3: 2.0})), (0, SparseVector(5, {1: 3.0, 4: 4.0})), (1, SparseVector(5, {2: 5.0, 3: 6.0}))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 19:45:09 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2358952 ms exceeds timeout 120000 ms\n",
      "24/08/06 19:45:09 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Inicializar SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Exemplo de dados\n",
    "data = [\n",
    "    (1, Vectors.sparse(5, {0: 1.0, 3: 2.0})),\n",
    "    (0, Vectors.sparse(5, {1: 3.0, 4: 4.0})),\n",
    "    (1, Vectors.sparse(5, {2: 5.0, 3: 6.0}))\n",
    "]\n",
    "\n",
    "# Criar RDD\n",
    "rdd = sc.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c1cab",
   "metadata": {},
   "source": [
    "### Resumo\n",
    "\n",
    "- **Vetores Densos**: Utilize-os em RDDs quando a maioria dos dados são não-zero, como em representações de características contínuas onde poucas entradas são zero.\n",
    "- **Vetores Esparsos**: Utilize-os em RDDs quando a maioria dos dados são zeros, como em representações textuais ou dados de alta dimensionalidade onde apenas algumas dimensões têm valores não-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594fc5c",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Pré-Processamento de Dados Para Construção de Modelos de Machine Learning\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Redução de Dimensionalidade com PCA\n",
    "\n",
    "A redução de dimensionalidade deve ser aplicada quando o número de vaiáveis preditoras for muito alto.\n",
    "\n",
    "A redução de dimensionalidade é uma técnica usada para reduzir o número de variáveis preditoras em um conjunto de dados. Isso é útil quando se trabalha com um grande número de variáveis, pois pode simplificar o modelo, reduzir o tempo de processamento e ajudar a evitar o overfitting (quando o modelo se ajusta demais aos dados de treinamento e não generaliza bem para novos dados).\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310df4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto PCA com 3 componentes\n",
    "bankPCA = PCA(k = 3, inputCol = \"features\", outputCol = \"pcaFeatures\")\n",
    "\n",
    "# Treina o modelo\n",
    "pcaModel = bankPCA.fit(bankDF)\n",
    "\n",
    "# Aplica o modelo PCA para reduzir a dimensionalidade\n",
    "pcaResult = pcaModel.transform(bankDF).select(\"label\", \"pcaFeatures\")\n",
    "\n",
    "# Visualiza o Resultado\n",
    "pcaResult.show(truncate = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
