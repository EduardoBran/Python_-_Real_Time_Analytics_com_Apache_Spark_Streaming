{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b78e15",
   "metadata": {},
   "source": [
    "# <span style=\"color: green; font-size: 40px; font-weight: bold;\">Real-Time Analytics com Apache Spark Streaming</span>\n",
    "\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510f9ec",
   "metadata": {},
   "source": [
    "# Batch vs Streaming\n",
    "\n",
    "Há dois tipos de dados quanto à forma como os dados são processados com Apache Spark: **Batch** e **Streaming**. O diagrama abaixo resume a diferença no processamento desses tipos de dados.\n",
    "\n",
    "### Batch\n",
    "\n",
    "- **Processamento de arquivos ou datasets finitos**: Você inicia o processamento de um arquivo ou dataset finito, o Spark processa as tarefas configuradas e conclui o trabalho.\n",
    "\n",
    "### Streaming\n",
    "\n",
    "- **Processamento de fluxos de dados contínuos (Stream)**: Você processa um fluxo de dados contínuo; a execução não para até que haja algum erro ou você termine a aplicação manualmente.\n",
    "\n",
    "<br>\n",
    "\n",
    "## Exemplos de Uso\n",
    "\n",
    "### Dados em Batch normalmente são usados para:\n",
    "- Análise exploratória de dados.\n",
    "- Processamento de dados para relatórios.\n",
    "- Treinar um modelo de aprendizado de máquina sobre grandes conjuntos de dados.\n",
    "- Análise de dados de Data Warehouses ou Data Lakes.\n",
    "- Outras tarefas analíticas que não precisam ser feitas em tempo real.\n",
    "\n",
    "### Dados em Streaming normalmente são usados para:\n",
    "- Monitoramento de serviços.\n",
    "- Processamento de eventos em tempo real para alimentar dashboards.\n",
    "- Processamento de dados de cliques e eventos em websites.\n",
    "- Processamento de dados de sensores da Internet das Coisas.\n",
    "- Processamento de dados vindos de serviços como Twitter, Kafka, Flume, AWS Kinesis.\n",
    "- Treinar um modelo de aprendizado de máquina sobre conjuntos de dados em tempo real.\n",
    "\n",
    "Portanto, a decisão de usarmos dados em Batch ou Streaming depende do caso de uso e do objetivo do projeto. Mas considere que qualquer tarefa que envolva dados em Streaming sempre terá um grau de complexidade um pouco maior, inerente às características desse tipo de dado e da velocidade de geração.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954b6f2",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Conhecendo o Apache Spark Structured Streaming\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### Structured Streaming (Streaming Estruturado)\n",
    "\n",
    "O **Structured Streaming** é um mecanismo de processamento de fluxo de dados (Streaming) escalável e tolerante a falhas, criado sobre o mecanismo **Spark SQL**.\n",
    "\n",
    "O Structured Streaming é uma versão atualizada do Spark Streaming, que foi otimizado nas versões mais recentes do Apache Spark. Daqui em diante, sempre que fizermos menção ao Spark Streaming, estaremos falando sobre o Spark Structured Streaming.\n",
    "\n",
    "Visite a documentação oficial: [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "\n",
    "Com o Structured Streaming, você pode expressar a computação de streaming da mesma forma que expressaria uma computação em Batch com dados estáticos. O mecanismo Spark SQL cuidará de executá-lo de forma incremental e contínua, e atualizará o resultado final à medida que os dados de streaming continuam chegando.\n",
    "\n",
    "Você pode usar Scala, Java, Python ou R para expressar agregações de streaming, janelas de tempo de evento, junções de fluxo para lote, etc. A computação é executada no mesmo mecanismo Spark SQL otimizado.\n",
    "\n",
    "Por fim, o Structured Streaming oferece garantias de tolerância a falhas de ponta a ponta por meio de pontos de verificação e logs. Resumindo, o Structured Streaming fornece processamento de fluxo de dados de forma rápida, escalável, tolerante a falhas e de ponta a ponta, sem que o usuário precise raciocinar sobre o streaming.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# O Que São DStreams (Discretized Streams)?\n",
    "\n",
    "Fique tranquilo que o conceito é bem mais simples que o nome. O Spark Streaming é um módulo do Apache Spark para processamento de dados em tempo real. E assim como os RDD’s (Resilient Distributed Datasets) são a base do Spark, os DStreams são a base do Spark Streaming. Vejamos o que são esses objetos.\n",
    "\n",
    "Um **DStream** é uma sequência de dados que são coletados ao longo do tempo. Internamente, cada DStream é representado por uma sequência de RDD’s coletados em cada unidade de tempo. Os DStreams podem ser criados a partir de diversas fontes como Flume, Kafka, HDFS ou Twitter. \n",
    "\n",
    "Uma vez que são criados, os DStreams oferecem 2 tipos de operações: transformações, que geram um novo DStream, e operações de output (ações), que gravam os dados em um sistema de armazenamento ou outra fonte externa.\n",
    "\n",
    "Os DStreams oferecem muitas das operações que podem ser realizadas com os RDD’s, mais as operações relacionadas ao tempo, como **sliding windows** (que veremos mais adiante). Feito o processamento, enviamos os dados para um sistema de armazenamento ou outro sistema de processamento.\n",
    "\n",
    "Um DStream é composto de diversos RDD’s e cada RDD contém dados em um intervalo de tempo. Se aplicamos uma operação ao DStream, estamos aplicando a mesma operação a todos os RDD’s que o compõem. Mas podemos também aplicar as operações aos RDD’s individualmente.\n",
    "\n",
    "#### Veja este exemplo abaixo. \n",
    "\n",
    "Aplicamos a transformação **flatMap** ao DStream chamado `lines`. Esta operação é então aplicada a cada RDD, gerando um novo DStream, com os RDD’s transformados. A exemplo dos RDD’s, quando aplicamos uma transformação ao DStream, geramos um novo DStream. Mas fique tranquilo, pois você não vai ter que se preocupar com isso. Faremos uma chamada a uma função e o Spark faz o restante por nós.\n",
    "\n",
    "O DStream permite converter um conjunto de dados contínuo em um conjunto discreto de RDD’s (**DStream** significa **Discretized Streams**).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# O que são Operações Window em Streaming de Dados\n",
    "\n",
    "Vamos compreender agora um dos mais importantes conceitos em processamento de dados em tempo real, o conceito de **Window** (janela). E isso não tem relação com o famoso sistema operacional.\n",
    "\n",
    "O Spark Streaming oferece o que chamamos de **computed windowed**, que pode ser traduzido como computação em uma janela de tempo. Este recurso é usado para aplicar operações de transformação sobre os dados em uma janela específica. O Streaming de dados é um dataset contínuo, que está sendo carregado o tempo todo, de forma intermitente. E como os dados são um fluxo contínuo, podemos querer visualizar e manipular dados em uma janela de tempo específica. Esse é o conceito do **windowing**.\n",
    "\n",
    "Veja nesta imagem abaixo que temos um DStream original. A cada intervalo de tempo, um RDD é criado. O DStream é um fluxo contínuo de dados, capturado do Twitter, por exemplo. No momento que criamos o Streaming Context, definimos qual será o intervalo de tempo. Entretanto, pode ser necessário manipular dados em um intervalo de tempo maior. Vamos supor que a cada um segundo, um RDD seja criado. Podemos definir uma **window** de 3 segundos e analisar os dados de 3 RDD’s neste exemplo. Simples, não?\n",
    "\n",
    "São duas configurações principais quando aplicamos windowing:\n",
    "\n",
    "- **Window length** – Duração da window (3 unidades de tempo por exemplo).\n",
    "- **Sliding interval** – Intervalo entre as windows.\n",
    "\n",
    "Abaixo temos uma linha de código com PySpark. Consegue compreender o que está sendo feito?\n",
    "\n",
    "`conta_palavras = dados.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)`\n",
    "\n",
    "Observe os valores `30` e `10` ao final da linha de código. Queremos contar as palavras durante 30 segundos de dados a cada 10 segundos. O valor `30` é a **window length** e o valor `10` é o **sliding interval**. Usamos a função `reduceByKeyAndWindow` com duas expressões lambda para aplicar a redução dos dados e contar as palavras no DStream chamado `dados`.\n",
    "\n",
    "No momento que criamos o Streaming Context, definimos ainda o **batch interval**, que é o intervalo de tempo de captura dos dados no Streaming. E podemos também definir os parâmetros tanto do tamanho da janela (**window length**) quanto o intervalo entre as janelas (**sliding interval**). Essas configurações são aplicadas ao DStream com as funções de windowing do Spark.\n",
    "\n",
    "Aqui estão as definições dos 3 parâmetros de tempo:\n",
    "\n",
    "- **Batch Interval**: Frequência com que os dados são capturados em um DStream.\n",
    "- **Sliding Interval**: Frequência com que uma window é aplicada.\n",
    "- **Window Interval**: Intervalo de tempo capturado para computação e geração de resultados.\n",
    "\n",
    "Resumindo. Suponha que seu **batch interval** seja de 1 segundo. A cada 1 segundo, geramos um RDD com dados do nosso Streaming. Podemos definir um **window interval** de 3 segundos, que vai conter 3 batches neste exemplo. O intervalo de tempo entre a geração das windows, é o que chamamos de **sliding interval**.\n",
    "\n",
    "Podemos até mesmo fazer redução por key e por window e, com isso, criar soluções analíticas de Streaming bem precisas.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# Usando SQL Para Processamento de Streaming de Dados\n",
    "\n",
    "Você pode usar DataFrames e operações SQL em dados de Streaming. Para isso, você precisa criar uma **SparkSession** usando o **SparkContext** que o **StreamingContext** está utilizando. Além disso, isso deve ser feito de forma que possa ser reiniciado em caso de falhas de driver. Isso é feito criando uma instância singleton do **SparkSession**. \n",
    "\n",
    "Isso é mostrado no exemplo fornecido pelo Spark no link abaixo. Você também pode executar consultas SQL em tabelas definidas em dados de Streaming de um thread diferente (ou seja, de forma assíncrona com o **StreamingContext** em execução). Apenas certifique-se de definir o **StreamingContext** para lembrar uma quantidade suficiente de dados de streaming para que a consulta possa ser executada. Caso contrário, o **StreamingContext**, que desconhece as consultas SQL assíncronas, excluirá os dados de Streaming antigos antes que a consulta possa ser concluída. \n",
    "\n",
    "Por exemplo, se você deseja consultar o último batch de dados, mas sua consulta pode levar 5 minutos para ser executada, chame a função `streamingContext.remember(Minutes(5))`. Quando trabalhamos com Streaming de dados, esse tipo de cuidado é sua responsabilidade.\n",
    "\n",
    "O Spark fornece um exemplo em Python sobre como aplicar SQL a um Streaming de dados: [Exemplo de SQL com Streaming](https://github.com/apache/spark/blob/v3.3.0/examples/src/main/python/streaming/sql_network_wordcount.py)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# Streaming Checkpoint\n",
    "\n",
    "Um aplicativo de streaming deve operar 24 horas por dia, 7 dias por semana e, portanto, deve ser resiliente a falhas não relacionadas à lógica do aplicativo (por exemplo, falhas do sistema, travamentos da JVM, etc.).\n",
    "\n",
    "Para que isso seja possível, o Spark Streaming precisa checar informações suficientes para que ele possa se recuperar de falhas. Existem dois tipos de dados que são verificados:\n",
    "\n",
    "1. **Checkpoint de Metadados** - Salvamento das informações que definem a computação de streaming para armazenamento tolerante a falhas, como HDFS. Isso é usado para se recuperar da falha do nó que executa o driver do aplicativo de Streaming. Os metadados incluem:\n",
    "   - **Configuração** - A configuração que foi usada para criar o aplicativo de Streaming.\n",
    "   - **Operações DStream** - O conjunto de operações DStream que definem o aplicativo de Streaming.\n",
    "   - **Lotes incompletos** - Lotes cujos trabalhos estão na fila, mas ainda não foram concluídos.\n",
    "\n",
    "2. **Checkpoint de Dados** - Salvamento dos RDDs gerados para armazenamento confiável. Isso é necessário em algumas transformações com estado que combinam dados em vários lotes. Em tais transformações, os RDDs gerados dependem dos RDDs de lotes anteriores, o que faz com que o comprimento da cadeia de dependência continue aumentando com o tempo. Para evitar esses aumentos ilimitados no tempo de recuperação (proporcional à cadeia de dependência), RDDs intermediários de transformações com estado são periodicamente verificados para armazenamento confiável (por exemplo, HDFS) para cortar as cadeias de dependência.\n",
    "\n",
    "Para resumir, o checkpoint de metadados é necessário principalmente para a recuperação de falhas de driver, enquanto o checkpoint de dados ou RDD é necessário mesmo para o funcionamento básico se forem usadas transformações com estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909b117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
