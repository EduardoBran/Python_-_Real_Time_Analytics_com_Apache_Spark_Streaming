{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a8b6a8",
   "metadata": {},
   "source": [
    "<span style=\"color: green; font-size: 40px; font-weight: bold;\">Projeto 1 (Análise de Dados em Tempo Real) </span>\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "# Análise de Dados de Sensores IoT (Internet das Coisas) em Tempo Real com Apache Spark Streaming e Apache Kafka\n",
    "\n",
    "<br>\n",
    "\n",
    "### Contexto\n",
    "\n",
    "Uma determinada indústria de materiais esportivos tem diversos equipamentos no parque industrial da empresa usados para produção e que funcionam 24/7.\n",
    "\n",
    "Cada equipamento tem um sensor que mede a temperatura em intervalos regulares. Os equipamentos que excedem uma determinada temperatura média por muito tempo, podem ter a vida útil reduzida, gerando custos adicionais de manutenção ou troca do equipamento.\n",
    "\n",
    "O departamento de operações gostaria de ter uma **solução de análise de dados em tempo real que calculasse a média de temperatura de cada equipamento a partir da leitura dos dados emitidos pelos sensores IoT em intervalos regulares**. Isso ajudaria no monitoramento da operação e ainda permitiria criar um histórico de uso dos equipamentos.\n",
    "\n",
    "Além de construir a solução com Spark e Kafka, vamos desenvolver um simulador para gerar dados de sensores IoT.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "O objetivo deste projeto é **demonstrar como configurar e executar uma pipeline de dados em tempo real que coleta, processa e analisa dados de sensores IoT utilizando Apache Kafka e Apache Spark Structured Streaming**. A análise se concentra em calcular a média das temperaturas reportadas por diferentes sensores em tempo real, permitindo monitorar e responder a condições específicas conforme os dados são gerados.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pergunta de Negócio Principal\n",
    "\n",
    "> A principal pergunta de negócio que este projeto visa responder é: \"**Qual é a temperatura média registrada por cada sensor IoT em tempo real, e como podemos monitorar e responder a temperaturas que excedem um determinado limiar?**\"\n",
    "\n",
    "<br>\n",
    "\n",
    "### Entregável\n",
    "\n",
    "O entregável deste projeto é uma aplicação de streaming em tempo real que:\n",
    "\n",
    "- Coleta dados de sensores IoT em tempo real usando Apache Kafka.\n",
    "- Processa e analisa esses dados em tempo real usando Apache Spark Structured Streaming.\n",
    "- Calcula e exibe a média de temperatura por sensor.\n",
    "- Permite a consulta em tempo real dos sensores que reportam temperaturas acima de um certo limite (por exemplo, acima de 65 graus Celsius).\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre a Fonte de Dados\n",
    "\n",
    "Os dados utilizados no projeto são gerados por sensores IoT que monitoram a temperatura (pequenos sensores em maquinas industrias que medem a temperatura das máquinas em tempos regulares). Cada entrada de dados inclui:\n",
    "\n",
    "- **id_sensor**: Identificador único do sensor.\n",
    "- **id_equipamento**: Identificador único do equipamento ao qual o sensor está conectado.\n",
    "- **sensor**: Nome ou tipo do sensor.\n",
    "- **data_evento**: Timestamp do evento de leitura.\n",
    "- **padrao**: Objeto que encapsula as leituras do sensor, neste caso, a temperatura.\n",
    "\n",
    "#### Exemplo de Entrada de Dados (json):\n",
    "\n",
    "<br>\n",
    "\n",
    "```\n",
    "{\n",
    "  \"id_sensor\": \"S-DSA-MP6-CAP15-02468-374DM\",\n",
    "  \"id_equipamento\": \"E-DSA-MP6-CAP15-13579-374DM\",\n",
    "  \"sensor\": \"sensor25\",\n",
    "  \"data_evento\": \"2022-11-05T15:22:16.968007Z\",\n",
    "  \"padrao\": {\n",
    "    \"formato\": \"iot:leitura:sensor:temp\",\n",
    "    \"leitura\": {\n",
    "      \"temperatura\": 42.0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Cada leitura de temperatura é capturada em um formato JSON e enviada para o tópico Kafka, que é então consumido pelo Spark Structured Streaming para análise em tempo real.\n",
    "\n",
    "#### Como simular isso?\n",
    "\n",
    "Precisamos encontrar uma forma de simular a geração de dados em tempo real a partires de sensores IoT. Para isso será necessário a **construção de um simulador usando a linguagem python para gerar dados de sensores IoT**.\n",
    "\n",
    "No dia a dia, bastaria solicitar os dados ao responsável pelas máquinas os arquivos gerados pelos sensores IoT.\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Código Implementado\n",
    "\n",
    "O código implementa as seguintes etapas:\n",
    "\n",
    "#### 1. Configuração do Ambiente:\n",
    "\n",
    "- Importação das bibliotecas necessárias, incluindo integração Spark-Kafka.\n",
    "- Criação da sessão Spark.\n",
    "\n",
    "#### 2. Leitura do Stream:\n",
    "\n",
    "- Usando\n",
    "- O código configura a leitura do stream a partir de um tópico Kafka chamado `dsamp6`, usando `spark.readStream.format(\"kafka\")` e outras opções como `kafka.bootstrap.servers` e `subscribe`.\n",
    "\n",
    "#### 3. Definição do Schema:\n",
    "\n",
    "- Dois esquemas são definidos:\n",
    "  - Um esquema para a leitura da temperatura (`esquema_dados_temp`), que modela a estrutura JSON esperada para a temperatura.\n",
    "  - Um esquema global (`esquema_dados`), que define a estrutura completa do JSON, incluindo identificadores de sensor, equipamento, e a leitura da temperatura.\n",
    "\n",
    "#### 4. Parse e Preparo dos Dados:\n",
    "\n",
    "- Conversão de cada linha de dado do stream para JSON e transformação em um DataFrame estruturado.\n",
    "  - O stream é convertido de `value` para string usando `CAST(value AS STRING)`.\n",
    "  - A string JSON é transformada em um DataFrame usando `from_json(col(\"value\"), esquema_dados)`, o que permite a extração dos campos individuais.\n",
    "  - O DataFrame resultante é preparado selecionando apenas os campos necessários (`temperatura` e `sensor`).\n",
    "\n",
    "#### 5. Análise de Dados em Tempo Real:\n",
    "\n",
    "- Cálculo da média de temperatura por sensor.\n",
    "- Exibição do resultado em tempo real no console.\n",
    "- Execução de consultas SQL em tempo real para monitorar sensores que excedem um determinado limiar de temperatura.\n",
    "\n",
    "- A análise calcula a média da temperatura por sensor usando `groupby(\"sensor\").mean(\"temperatura\")`.\n",
    "- As colunas são renomeadas para simplificar a análise (`sensor`, `media_temp`).\n",
    "- A query de streaming é executada em modo `complete` e imprime o resultado no console em tempo real.\n",
    "\n",
    "#### 6. Persistência e Visualização dos Resultados:\n",
    "\n",
    "- Outra query de streaming é configurada para manter os resultados em memória (`format(\"memory\")`).\n",
    "- SQL é usado para consultar os dados em tempo real, filtrando sensores com temperaturas acima de 65 graus.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Considerações Finais\n",
    "\n",
    "Este mini-projeto demonstra como é possível utilizar ferramentas modernas de big data para implementar soluções de análise em tempo real. A combinação de Apache Kafka e Apache Spark Structured Streaming oferece uma solução robusta e escalável para lidar com fluxos de dados contínuos, como os gerados por dispositivos IoT. Através desta pipeline, é possível monitorar, analisar e reagir aos dados à medida que são gerados, fornecendo insights imediatos e acionáveis para o negócio.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "# Instruções para executar o projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Etapa 1 - Simulador IoT\n",
    "\n",
    "1. Abra o terminal ou prompt de comando e acesse a pasta do projeto e vá para a pasta `simulador_iot` que contém o script `simulador.py`. Esta pasta é onde o simulador IoT está localizado, e esse script foi desenvolvido para gerar leituras simuladas de sensores IoT.\n",
    "\n",
    "> **O que o script faz:** O script `simulador.py` gera dados simulados de sensores de temperatura em formato JSON. Ele atribui valores de temperatura a sensores fictícios e os salva em um arquivo de saída. Esses dados são então usados no restante do projeto para simular um fluxo de dados IoT em tempo real.\n",
    "\n",
    "2. Execute o comando abaixo para gerar um arquivo com 10.000 leituras de sensores IoT (você pode ajustar o número de registros conforme desejar).\n",
    "\n",
    "   `python simulador.py 10000 > ../dados/dados_sensores.txt`\n",
    "\n",
    "### Etapa 2 - Apache Kafka\n",
    "\n",
    "**O que é o Apache Kafka:** \n",
    "\n",
    "O Apache Kafka é uma plataforma de streaming distribuída que permite publicar, subscrever, armazenar e processar fluxos de registros em tempo real. Neste projeto, o Kafka atua como uma ponte entre a fonte de dados (sensores IoT) e o Spark Streaming, permitindo que os dados de sensores sejam capturados e transmitidos para processamento em tempo real.\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Acesse a página do Kafka e faça o download da versão usada no curso conforme mostrado na aula em vídeo.\n",
    "\n",
    "2. Descompacte o arquivo do Kafka dentro da pasta do Mini-Projeto 6.\n",
    "\n",
    "   **Nota:** As instruções abaixo são para MacOS e Linux. Para Windows as instruções estão no manual em pdf no Capítulo 15 do curso.\n",
    "\n",
    "3. Abra o terminal, navegue até a pasta do Kafka e execute o comando abaixo para inicializar o Zookeepper (gerenciador de cluster do Kafka).\n",
    "\n",
    "   `bin/zookeeper-server-start.sh config/zookeeper.properties`\n",
    "\n",
    "4. Abra outro terminal, navegue até a pasta do Kafka e execute o comando abaixo para inicializar o Kafka.\n",
    "\n",
    "   `bin/kafka-server-start.sh config/server.properties`\n",
    "\n",
    "5. Abra outro terminal, navegue até a pasta do Kafka e execute o comando abaixo para criar um tópico no Kafka.\n",
    "\n",
    "   `bin/kafka-topics.sh --create --topic dsamp6 --bootstrap-server localhost:9092`\n",
    "\n",
    "6. No mesmo terminal anterior, execute o comando abaixo para descrever o tópico.\n",
    "\n",
    "   `bin/kafka-topics.sh --describe --topic dsamp6 --bootstrap-server localhost:9092`\n",
    "\n",
    "7. No mesmo terminal anterior, execute o comando abaixo para produzir o streaming de dados no Kafka (como um produtor de streaming).\n",
    "\n",
    "   `bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic dsamp6 < ../dados/dados_sensores.txt`\n",
    "\n",
    "8. No mesmo terminal anterior, execute o comando abaixo para listar o conteúdo do tópico (como um consumidor de streaming).\n",
    "\n",
    "   `bin/kafka-console-consumer.sh --topic dsamp6 --from-beginning --bootstrap-server localhost:9092`\n",
    "\n",
    "9. Pressione `Ctrl+C` a qualquer momento para interromper qualquer uma das janelas. Mantenha todas elas abertas enquanto executa a Etapa 3 do projeto.\n",
    "\n",
    "### Etapa 3 - Apache Spark\n",
    "\n",
    "1. Execute o Jupyter Notebook do projeto e execute célula a célula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad31a85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f96c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77275eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49c14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf41f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b625a6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbcb194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074217b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2deeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb302578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd79ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2fea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75260790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
